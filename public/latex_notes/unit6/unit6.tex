\include{./../latex/notes_style.tex}


\setcounter{unit}{1}
\setcounter{section}{0}


\begin{document}



\title{Unit 6: regularization, priors and Bayesian inference}
\author{Ethan Levien}
\maketitle


\section{Introduction to Bayesian inference: Simple examples}

So far, we think of models encode our assumptions about how data was generate. Our models depend on parameters for which the true values are fixed. When use data to \dfn{fit} or \dfn{infer} parameters, obtain a sample distribution which roughly speaking quantifies uncertainty in our estimates. This way of performing statistics, where the uncertainty is thought of as a distribution over repeated experiments, is \dfn{frequentist} approach to statistics.  In this formulation of statistical inference we are pretending to have complete ignorance of the parameters before we see the data, but in reality, this is never true. For example, if we flip a coin $N=3$ times and get $Y=3$ heads, our estimate of the probability this coin will land on heads in the next flip is $\hat{q} = 1$, which is clearly not in line with out understanding that both sides are at least possible. It is also worth noting that the usual estimate standard error, ${\rm se}(\hat{q}) = \sqrt{q(1-q)/N} \approx  \sqrt{\hat{q} (1-\hat{q} )/N}$ gives zero, but this is obviously not a good estimate of the uncertainty. 

Similar issues emerge in the context of more machine learning-style data analysis, sometimes we want to incorporate vague information, such as ``the function describing my data is very smooth and changes roughly on a time-scale of 5 hours". Or, we might want to include many predictors, but to avoid overfitting penalize high values of the predictors. For example, we might believe there is an interaction term, but suspect it is much smaller than the additive terms. 

 While there are ways handle these problems within the frequentist framework, they are handled more naturally by taking an entirely different treatment of parameters and statistical inference. This is called \dfn{Bayesian statistics}. In the Bayesian formulation of statistics, we will think of parameters as themselves random variables which are given a distribution before we have seen the data. Mathematically, this means instead of having a model with fixed parameter: 
 \begin{equation}
X \sim {\rm ModelDistribution}(\theta)
 \end{equation}
 we think of our model as a conditional on $\theta$
  \begin{equation}
X|\theta \sim {\rm ModelDistribution}(\theta). 
 \end{equation}
We call $X|\theta$ the \dfn{likelihood} (this term is used in both frequentists and Bayesian approaches). 
Then, we add a new distribution for $\theta$, called the prior. The goal is then to condition on the observed data $X_1,\dots,X_N$ to find a new distribution $\theta|X_1,\dots,X_N$, called the posterior. The posterior plays a similar role to the sample distribution, and naturally we often use its mean as an estimate of $\theta$. However, there are both philosophical and mathematical differences between these two approaches which make the comparison imperfect. We will focus less on the philosophical aspect and see what is practically different about the approaches. This is best illustrated with the following example. 
 
 

\subsection{Bayesian inference for Bernoulli trails}

Consider the model 
\begin{equation*}
X \sim {\rm Bernoullli}(q)
\end{equation*}
for the outcome of a coin flip. If we perform $N$ flips, then recall $Y = \sum_{i=1}^NX_i$ is binomial. 
In a Bayesian model, we would specify the prior distribution of $q$. This should reflect what we know about the coin, which in reality is that it is very likely to be fair. 
However, to make calculations simpler and better compare the frequentist approach we will take a prior where any value of $q$ is equally likely. This seems to be more similar to what we are doing in the frequentist approach. Our Bayesian model would then me
\begin{align*}
q &\sim {\rm Uniform}(0,1)\\
X|q &\sim {\rm Bernoullli}(q)
\end{align*}
%Another way to think about constructing an estimator of $q$, is that given data points $X_1,\dots,X_N$, we would like to know 
%\begin{equation}
%\hat{q}_b = E[q|X_1,\dots,X_N]\label{eq:qhatb}
%\end{equation}
%We can in principle solve this problem using Bayes' theorem. In fact, we would also take it a step further and ask: What is the distribution of
Our goal is to find the posterior distribution 
\begin{equation*}
q|X_1,\dots,X_N
\end{equation*}
Since the flips out independent, this distribution depends only on $Y$, so we can alternative obtain the posterior as the distribution of $q|Y$. 
Recall that the density of the uniform is simply $f(q) = 1$ (I'll use $f$ for a density and $P$ for probabilities, but sometimes we have to mix them). Using Bayes' theorem (or simply using the definition of conditional probability twice), we have 
\begin{equation}
f(q|Y=y) = \frac{f(q,\{Y=y\})}{P(Y=y)} = \frac{P(Y=y|q)f(q)}{P(Y=y)} 
\end{equation}
where $f(q,\{Y=y\})$ means the joint density/probability of $q$ and $Y$. 
Recall the binomial distribution is 
\begin{equation}
P(Y=y|q) = {N \choose y}q^y(1-q)^{N-y}
\end{equation}
The important thing to realize is that all the $q$ dependence comes from $P(Y=y|q)$ and since our goal is to find a distribution of $q$, we can write
\begin{equation}
f(q|Y=y) = C(N,y)q^y(1-q)^{N-y}
\end{equation}
where $C(N,y)$ is a constant which ensures $\int f(q|Y=y)dq = 1$. That is, 
\begin{equation}
C(N,y)^{-1} = \int_0^1 q^y(1-q)^{N-y} dq
\end{equation} 
$f(q|Y=y)$ is an example of \dfn{Beta distribution}. More generally, we say a variable $q$ is
\begin{equation}
q \sim {\rm Beta}(a,b)
\end{equation}
if $q$ has the density 
\begin{equation}
f(q) = q^{a-1}(1-q)^{b-1}
\end{equation}
hence $q|Y \sim {\rm Beta}(y+1,N-y+1)$. 
 You don't need to memorize this, but you should look on Wikipedia or in the colab notebook to get a sense of what this function looks like. 




\begin{example}[Laplace Rule of Succession of Bayesian view]\label{ex:beta}

Consider Bayesian inference for Bernoulli trials with uniform priors as described above. Recall that Laplace's Rule of Succession gives the estimator 
\begin{equation}
\hat{q}_L = \frac{Y+1}{N+2}
\end{equation}


\noindent
\underline{Question}: Show that when $Y=N$
\begin{equation*}
E[q|Y] = \frac{N+1}{N+2} = \hat{q}_L
\end{equation*}

\noindent
\underline{Solution}: 
In this case 
\begin{equation}
C(N,y)^{-1} =C(N,N)^{-1} = \int_0^1 q^N dq = \frac{1}{N+1}
\end{equation} 
and 
\begin{equation}
E[q|Y]  = (N+1) \int_0^1 q \times q^{N}dq = (N+1) \int_0^1 q^{N+1}dq  = \frac{N+1}{N+2}. 
\end{equation}


%It turns out that this has a relatively simple form, called a $\beta$-distribution. We say 
%\begin{equation*}
%q \sim {\rm Beta}(a,b)
%\end{equation*}
%if $q$ is a random variable on $[0,1]$ which has density 
%\begin{equation*}
%f(q) = Bq^{a-1}(1-q)^{b-1}
%\end{equation*}
%The constant $B$ ensures the area under this curve between $q=0$ and $q=1$ is indeed one, as it must be for a random density. \\
%



%\noindent
%\underline{Solution}: Using Bayes' theorem 
%\begin{align*}
%f(q|Y) &= \frac{P(Y|q)f(q)}{P(Y)}\\
%&=  \frac{1}{P(Y)}{N \choose Y}q^{Y}(1-q)^{N-Y}
%\end{align*}
%Since we are thinking of this as a probability density in $q$, we don't actually need to compute $P(Y)$ explicitly (even though we could), instead, we can simply notice that 
%\begin{equation*}
%f(q|Y)  = Cq^{Y}(1-q)^{N-Y}
%\end{equation*}
%for some constant of proportionality $C$ which will depend on $Y$, but is uniquely determined by the fact that the area under this curve must be $1$.  In the colab notebook we plot this curve. 
\end{example}


 To summarize the idea of Bayesian inference, If $Y$ is a our data and $\theta$ is our parameter(s) ($q$ in the example above) Bayes' theorem tells us that 
\begin{equation}\label{eq:bayes}
P(\theta|Y) = \frac{P(Y|\theta)P(\theta)}{P(Y)}
\end{equation}



 The distributions appearing in Equation \eqref{eq:bayes} have the following names and interpretations:
\begin{itemize}
\item $P(\theta|Y)$ is the \dfn{posterior} (a beta distribution in Example \ref{ex:beta}).
\item $P(Y|\theta)$ is the \dfn{likelihood} (Binomial distribution in Example \ref{ex:beta}).
\item $P(\theta)$ is the prior distribution (Uniform distribution in Example \ref{ex:beta})
\item $P(Y)$ is the evidence. It represents the chance we observe the data \emph{unconditional} on the parameters. This can be obtained by marginalizing over the priors.
\end{itemize}


While in classical statistics, the objective is to determine (estimate) a parameter $\theta$ and measure its uncertainty (with the sample distribution), in Bayesian statistics our objective is to compute the posteior distribution. Once we have this, we can obtain so-called \dfn {point estimates}, for example, by taking the average of $\theta$ or maximum of $P(\theta|Y)$ (maximum likelihood).  The point estimates are however less central in Bayesian inference than the posterior. 

\subsection{Selecting priors with a beta distribution (optional)}
If
\begin{equation*}
q \sim {\rm Beta}(a,b)
\end{equation*}
then (using calculus) it can be shown that 
\begin{align}\label{eq:betameanvar}
E[q] &= \frac{a}{a+b}\\
{\rm var}(q) &= \frac{ab}{(a+b)^2(a+b+1)}.
\end{align}
Moreover, you can check (by plotting the density in Python -- see exercises) that when $a$ and $b$ become large the beta distribution approaches a Normal distribution. 


\begin{example}[Selecting priors]\label{ex:beta2}
Suppose we are given a coin. We are trying to determine whether it is biased based on the outcome of $N$ flips. We are pretty sure that, like most coins, it is not biased. To be precise, let's say you are $95\%$ confident that the bias of the coin is less than $10\%$ biased towards heads or tails.\\

\noindent 
\underline{Question}: 
\begin{enumerate}[label=(\alph*)]
\item Select a prior distribution for $q$. 
\item What is the posterior expectation if we flip the coin $5$ times and see $5$ heads? \\
\end{enumerate} 


\noindent 
\underline{Solution}: 
\begin{enumerate}[label=(\alph*)]
\item We know that a $\beta$-distribution is approximately Normal if $a$ and $b$ are large enough. Assuming we can make a Normal approximation, we can use the formulas for the mean and variance of the $b$ distribution to select parameters such that 
\begin{equation}\label{eq:Pqpriors}
P(|q-0.5|<0.1)  = P(0.4<q<0.6) \approx 0.95
\end{equation}
Since we would like the mean of our prior distribution to be $1/2$, we select
\begin{equation*}
q \sim {\rm Beta}(a,a).
\end{equation*}
If $q$ is approximately Normal, then Equation \ref{eq:Pqpriors} will hold when 
\begin{align*}
1.96 \sqrt{{\rm var}(q)} &= 0.1 \\
&\implies  \sqrt{\frac{a^2}{4a^2(2a+1)}} = \frac{1}{2}\sqrt{\frac{1}{2a+1}}= \frac{0.1}{1.6}\\
&\implies a \approx 31.5
\end{align*}
\item In order to compute the posterior, we use Bayes' theorem 
\begin{equation*}
f(q|X_1,\dots,X_N) \propto q^{Y}(1-q)^{N-Y} \times q^{a-1}(1-q)^{a-1} = q^{Y+a-1}(1-q)^{N-Y+a-1}
\end{equation*}
where $Y = \sum_{i=1}^NX_i$. 
Again, the $q$ dependence uniquely determines the distribution, since the area under this curve must be one. We can therefore say that 
\begin{equation*}
q|X_1,\dots,X_N \sim {\rm Beta}(Y+a,N-Y+a)
\end{equation*}
For the mean and variance we have
\begin{align*}
E[q|X_1,\dots,X_N] &= \frac{Y+a}{N+2a} =  \frac{5+31.5}{5+2\times 31.5}  \approx 0.54
\end{align*}
Note that if we used usual estimator we would have found $\hat{q} = 1$, while if we used uniform priors and taken the posterior expectation $\hat{q}_L = 0.85$. Yet another approach would be to take the posterior maximum (the mode) of the posterior in using the $\beta$-distribution priors. 

\end{enumerate}

\end{example}

% Something quite nice about using $\beta$-distribution priors for the Bernoulli model is that BOTH the prior and the posterior have a $\beta$-distribution, albeit with difference parameters. In general, when this is the case we say that the priors are \dfn{conjugate}. 

\subsection{Bayesian inference for Normal mean}

Suppose we have a Normal model for $X$, but treat the noise variance as a constant and focus on Bayesian inference of $\mu$. 
\begin{equation*}
X|\mu \sim {\rm Normal}(\mu,\sigma^2/N).
\end{equation*}
and
\begin{equation*}
\bar{X}|\mu \sim {\rm Normal}(\mu,\sigma^2/N).
\end{equation*}
It is natural to take Normal priors on $\mu$, since then the marginal of $\bar{X}$ is Normal (you should check this!) making our calculations much easier. Hence we set 
\begin{equation*}
\mu \sim {\rm Normal}(m,s^2)
\end{equation*}
Notice that this is nothing but a linear regression model for $\bar{X}$ with $\mu$ as a predictor (and regression slope $\beta =1$). Therefore, the computation of the posterior $\mu|\bar{X}$ amounts to ``inverting'' a linear regression model, something we are familiar with from a number of examples and exercises in previous units. 

In particular, since $\mu|\bar{X}$ is Normal, we only need to calculate the mean and variance.  
\begin{align}
m &= E[\mu] =  b_1 E[\bar{X}] + b_0\\
s^2 &= {\rm var}(\mu) =b_1^2 {\rm var}(\bar{X}) + {\rm var}(\mu|\bar{X})
\end{align}
where $b_1$ and $b_0$ are the regression slope and intercept when $Y$ is treated as the predictor. In particular, $b_0 = E[\mu|\bar{X}=0]$. 

The variance and expected value of $Y$ are
\begin{equation}
E[\bar{X}] = m,\quad {\rm var}(\bar{X}) = s^2 + \sigma^2/N. 
\end{equation} 
The regression slope $b$ appearing in the above formula can be found from the covariance. Using the relation ${\rm cov}(X,Y) = \beta_1 \sigma_X^2$ (here $X$ is $\mu$ and $\beta_1=1$), we have
\begin{equation}
{\rm cov}(\mu,\bar{X}) = s^2
\end{equation}
hence the ``inverse'' regression slope is obtained by the usual regression slope relation: 
\begin{equation}
s^2 = {\rm cov}(\mu,\bar{X}) = b_1(s^2 + \sigma^2/N) \implies b_1= \frac{s^2}{s^2 +\sigma^2/N}
\end{equation}
Coming these facts yields 
\begin{equation}
b_0 = m\left(1 -  \frac{s^2}{s^2 +\sigma^2/N}\right) = m \frac{\sigma^2/N}{s^2 +\sigma^2/N}
\end{equation}
Finally 
\begin{align*}
{\rm var}(\mu|\bar{X}) &= \frac{s^2}{N}  - b_1^2 {\rm var}(Y) = \frac{s^2}{N} -  \frac{ s^2 + \sigma^2/N}{(1 + \left(\frac{\sigma}{s}\right)^2\frac{1}{N})^2}\\
&\quad \vdots \quad (\text{some algebra})\\
&= \frac{s^2\sigma^2}{s^2N + \sigma^2}
\end{align*}
In summary, the posterior distribution is 
\begin{equation}
\mu|\bar{X} \sim {\rm Normal}\left(b_1\bar{X} +m (1-b_1), \frac{s^2\sigma^2}{s^2N + \sigma^2}\right).
\end{equation}
Take note of what happens when $\sigma/s$ is very large/small. 


 

%----------------------------------------------------------------------------------------------------------------
\section{Bayesian inference for linear regression model with a single-predictor}\label{sec:bayeslr}
Now we consider a linear regression model with a single predictor. To make things simpler, we assume $\sigma_{\epsilon}^2$, $\mu_X = \mu_Y =0$ (that is, zero marginal expectations for the predictor and response variables). We will take mean zero Normal priors on the regression coefficient, so the model is 
\begin{align}
\beta &\sim {\rm Normal}(0,\tau^2)\\
Y|X,\beta &\sim {\rm Normal}(\beta X,\sigma_{\epsilon}^2)
\end{align}
The assumption that the mean of the prior distribution is zero can be relaxed without too much effort, but it will make the calculations a bit simpler. 
The motivation for taking Normal priors on $\beta$ is that the posterior will be Normal, and therefore we only need to calculate the mean and variance. In this case, the posterior distribution is obtained by conditioning $\beta$ on all the $X$ and $Y$ points 
\begin{equation}
\beta|(X_1,Y_1),\dots,(X_N,Y_N)
\end{equation}
but under the assumption we have made, this is the same as conditioning on the OLS estimator $\hat{\beta}$ and the $X$ points. 

Note that because we have assumed $X$ and $Y$ are mean zero, $\beta = {\rm cov}(X,Y)/{\rm var}(X) = E[XY]/E[X^2]$ and hence
\begin{equation}
\hat{\beta} = \frac{\sum_{i=1}^NX_iY_i}{\sum_{i=1}X_i^2}. 
\end{equation}
Since the $Y_i$ are Normal, $\hat{\beta}|(\beta,X_1,\dots,X_N)$ is Normal and has mean and variance
\begin{align}
E[\hat{\beta}|\beta X_1,\dots,X_N]&= E\left[\frac{\sum_{i=1}^NX_iY_i}{\sum_{i=1}X_i^2}  \right] =\frac{\sum_{i=1}^NX_i E[Y_i|\beta,X_i]}{\sum_{i=1}X_i^2}\\
&= \frac{\sum_{i=1}^NX_i E[Y_i|\beta,X_i]}{\sum_{i=1}X_i^2} =  \frac{\sum_{i=1}^NX_i^2 \beta}{\sum_{i=1}X_i^2} = \beta\\
{\rm var}(\hat{\beta}|\beta X_1,\dots,X_N) &= \frac{\sum_{i=1}^NX_i^2 {\rm var}(Y_i|\beta,X_i)}{\left(\sum_{i=1}X_i^2\right)^2}\\
&=  \frac{{\rm var}(Y_i|\beta,X_i)}{\sum_{i=1}X_i^2} = \frac{\sigma_{\epsilon}^2}{N\hat{\sigma}_X^2}
\end{align}
Therefore, 
\begin{equation}
\hat{\beta}|\beta,X_1,\dots,X_N \sim {\rm Normal}(\beta,\sigma_{\epsilon}^2/(N\hat{\sigma}_X^2)). 
\end{equation}
You might recognize the mean and variance from the sample distribution of the OLS estimator. Indeed, they are the same! But now we can use this to find the posterior distribution of $\beta$ conditioned on the OLS estimator $\hat{\beta}$. 
The key is that this is a linear regression model with $\beta$ as the predictor and $\hat{\beta}$ as the response variable!  If we ``invert'' the linear regression model as we have done before (you should fill in the details for yourself -- see exercises), we get
\begin{equation}\label{eq:bayes_lr_posterior}
\beta|(\hat{\beta},X_1,\dots,X_N) \sim {\rm Normal}\left( \hat{\beta}\frac{\tau^2}{\tau^2 + \sigma_{\epsilon}^2/(N\hat{\sigma}_X^2)},\frac{\sigma_{\epsilon}^2/(N\hat{\sigma}_X^2)\tau^2}{\tau^2 + \sigma_{\epsilon}^2/(N\hat{\sigma}_X^2)}\right)
\end{equation}



%----------------------------------------------------------------------------------------------------------------
\section{Bayesian inference for nonlinear model}
  We now discuss Bayesian inference for the general linear regression model with features. Let
\begin{equation*}
f(x) = \sum_{i=1}^K\beta_i\phi_i(x)
\end{equation*}
and suppose that our priors are 
\begin{equation*}
\beta_i \sim {\rm Normal}(0,\tau_i^2).
\end{equation*}
We will assume that $E[\phi_i(X)]=0$. We will also assume, for simplicity, that {\bf $\sigma$ is known. Hence, we do not need to consider estimating it.} This simplifies the conceptual picture considerably. 

 In this case, the Posterior distribution of the $\beta_i$ can be understood analytically. It turns out that the marginal posterior distribution of each $\beta_i$ is again Normal -- thus Normal priors on $\beta_i$ are conjugate priors!. The marginal mean of each $\beta_i$ are determined by the system of questions given in the following Theorem. 


\begin{thm}[Posterior mean for regression coefficients]\label{thm:regpostmean}
Define a $K\times K$ matrix $\Omega$ called the \dfn{empirical covariance matrix} which has entries
\begin{equation*}
\Omega_{i,j} = N\overline{\phi_i(X)\phi_j(X)} = \sum_{k=1}^N\sum_{z=1}^N\phi_i(X_k)\phi_j(X_z) 
\end{equation*}
and let $\bar{\beta}_i$ denote the posterior mean of $\beta_i$; that is, 
\begin{equation*}
\bar{\beta}_i =  E[\beta_i|y,X]. 
\end{equation*}
Then $\bar{\beta}_1,\dots,\bar{\beta}_K$ satisfy the $K$ equations 
\begin{equation}\label{eq:barbeta}
 \left(\frac{\sigma}{\tau_i}\right)^{2}\bar{\beta}_i  +\sum_{j=1}^K\Omega_{i,j}\bar{\beta}_j = \sum_{j=1}^N\phi_i(X_j)Y_j
\end{equation}

\end{thm}
We make a few remarks on this Theorem:
\begin{itemize}
\item If $N> K$, then $\bar{\beta}_i$ actually have a solution. In-fact, depending on $\Omega$, there may not be a solution. For our discussion, we will assume there is however. 
\item Under the assumption that $E[\phi_i(X)]=0$, the entries of $\Omega_{i,j}$ approximate the covariances between the predictor features. Moreover, if we take the predictors to be drawn from some distribution and average over the data (both $X$ and $Y$), we get
\begin{equation*}
\frac{E[\Omega_{i,j}]}{N}=  {\rm cov}(\phi_j(X),\phi_i(X)).
\end{equation*}
and  
\begin{equation*}
\frac{1}{N}\sum_{j=1}^NE[\phi_i(X_j)Y_j]= {\rm cov}(Y,\phi_i(X)).
\end{equation*}
Hence, we have the ``math world'' version of Equations \ref{eq:barbeta}, which is obtained by averaging over the data: 
\begin{equation*}
 \frac{1}{N}\left(\frac{\sigma}{\tau_i}\right)^{2}E[\bar{\beta}_i]  + \sum_{j=1}^K{\rm cov}(\phi_i(X),\phi_j(X))E[\bar{\beta}_j] = {\rm cov}(Y,\phi_i(X)).
\end{equation*}
In the limit $N \to \infty$ and with $K=2$, we obtain a system of equations recognizable from Week 5 notes for the regression coefficients in the two predictor model in terms of the covariances. In this context, the role of the regression coefficients (which we previously took to be fixed numbers), is now played by the averages $E[\bar{\beta}_j]$, which are the average values of $\beta_j$ with respect to both the posterior and the data. 
\item Notice that the first term in  Equation \ref{eq:barbeta} vanishes as either $\tau_i \to \infty$ (very broad priors) or $\sigma \to 0$ (no variance in $Y$ conditional on the predictors). In both cases, the vales of $\bar{\beta}_i$ are entirely determined by the data, and the priors play no role. When $\tau_i \to 0$ or $\sigma \to \infty$, the priors entirely determine determine $\bar{\beta}_i$ and in fact $\bar{\beta}_i =0$. 
\item The first term in  Equation \ref{eq:barbeta} is an example \dfn{regularization}. 
\end{itemize}


 Theorem \ref{thm:regpostmean} can be elegantly stated using matrices (see Linear algebra review note for intro to matrix multiplication).  This is import if we want to implement these computations in Python. Observe that one way to construct $\Omega_{i,j}$ is to define the matrices
\begin{align*}
A &= \left[ 
\begin{array}{cccc}
\vertbar & \vertbar& & \vertbar\\
\phi_1(X) & \phi_2(X) & \cdots & \phi_K(X) \\
\vertbar & \vertbar& & \vertbar
\end{array}
\right],\quad 
\Lambda_0 = \left[
\begin{array}{cccc}
\tau_1 & 0 & \dots & 0 \\
0 & \tau_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \tau_K \\
\end{array}
\right]
\end{align*}
Then $\Omega = A^TA$. 
Now define another matrix $K\times K$ matrix 
\begin{equation*}
M = \Omega + \sigma^2\Lambda_0^{-1}
\end{equation*}
Then Equations \ref{eq:barbeta} can be written as 
\begin{equation*}
M \bar{\beta} = A^Ty
\end{equation*}
where $\bar{\beta}$ and $y$ are respectively a $K$-vector and $N$-vector
\begin{equation*}
\bar{\beta} = \left[\begin{array}{c} \bar{\beta}_1\\ 
\vdots\\
\bar{\beta}_K \end{array}\right],\quad y = \left[\begin{array}{c}Y_1\\ 
\vdots\\
Y_N \end{array}\right]. 
\end{equation*}
Then $\bar{\beta}$ satisfies  
\begin{equation*}
M\bar{\beta} = A^Ty  \implies \bar{\beta} =  \left( A^TA+ \sigma^2\Lambda_0^{-1}\right)^{-1}A^Ty
\end{equation*}



\begin{example}[Calculating posterior mean in python]\label{ex:regpostmean}

Consider the model with two predictors:
\begin{equation*}
f(x) = \beta_1X_1 + \beta_2X_2\\
\end{equation*}


\noindent 
\underline{Question}: Generate some simulated data and compute the posterior means of $\beta$ for different choices of $\tau$. \\


\noindent 
\underline{Solution}: See  \href{https://colab.research.google.com/drive/1sFlY0nvo7hrsNYGRdx8C4tKPihyOSmbx?usp=sharing}{colab notebook}. 

\end{example}




\subsection{Posterior prediction}


 We will mostly focus on how we can use the posterior estimates to make predictions, although there is an entirely different set of questions we could ask concerning model assessment and hypothesis testing.Recall that $\hat{y}(x,D)$ is defined as our prediction, using the regression model fitted to the data $D$, of $E[Y|X=x]$. In the Bayesian context, we can define the posterior predictive variable $Y|X,D$ as the random representing the distribution of our response variable conditioned on the data. We can obtain this distribution by replacing the $\beta_i$s in 
\begin{equation*}
Y = \sum_{i=1}^{K}\beta_i\phi_i(X)
\end{equation*}
with samples from the posterior distribution. In the Bayesian framework we set
\begin{equation*}
\hat{y}(x,D) = E[Y|D,X] = \sum_{i=1}^{K}E[\beta_i|D]\phi_i(X) = \sum_{i=1}^{K}\bar{\beta}_i\phi_i(X) 
\end{equation*}
We can write this in matrix notation by defining the vector 
\begin{equation*}
a = \left[\begin{array}{c}
\phi_1(X)\\
\vdots\\
\phi_K(X)
\end{array}\right].
\end{equation*}
Then 
\begin{equation}\label{eq:yhatmatrixform}
\hat{y}(x,D) = a^T\bar{\beta} = a^TM^{-1}A^Ty
\end{equation}


\begin{example}[Posterior predictions for a regression model]\label{ex:fourier regularization}
Take the Fourier model
\begin{equation*}
f(x) =\beta_0 +  \sum_{i=1}^K\beta_i \sin\left(\frac{2\pi i x}{L} \right) + \alpha_i \cos\left(\frac{2\pi i x}{L} \right).
\end{equation*}
Implement a function to compute posterior predictions with different priors $\tau$ on the coefficients. In this example, we will fit this model to data generated by adding noise to the function 
\begin{equation*}
f_{\rm true}(x) = \sin(2\pi 3x) + 5\sin(2\pi 6x^2) + 20x^2
\end{equation*}
with $x \in [0,1]$.
Note that for $K<\infty$, we can not pick any values of the $\beta_i$s and $\alpha_i$s such that $f = f_{\rm true}$.  \\ 

We will assume that $\beta_i$ and $\alpha_i$ have the same prior distribution: 
\begin{align*}
\beta_i \sim {\rm Normal}(0,\tau_i)\\
\alpha_i \sim {\rm Normal}(0,\tau_i)\\
\end{align*}

\noindent 
\underline{Question}: 
Compute the posterior mean $\hat{y}(x,D)$ for the following choices of priors and $K=50$. 
\begin{enumerate}[label=(\alph*)]
\item All $\tau_i$ are the same, $\tau_i = \tau_0$. 
\item The $\tau_i$ decay with $i$ as $\tau_i = \tau_0/i$
\end{enumerate}
In each case, experiment with different choices of $\tau_0$ and pay attention to whether the model is overfitting (high variance, low bias) or under fitting (low variance, high bias). \\


\noindent 
\underline{Solution}: See  \href{https://colab.research.google.com/drive/1sFlY0nvo7hrsNYGRdx8C4tKPihyOSmbx?usp=sharing}{colab notebook}. 


 
\end{example}




%------------------------------------------------------------------------------------------------------------------------
\section{Regularization view of priors}
\begin{itemize}
\item Recall that in standard least squares linear regression, the regression coefficients come from minimizing the sum of squared residuals. That is, they are the values that minimize the function 
\begin{equation*}
L(\beta) = \sum_{j=1}^N(\hat{y}(X_j,D)- Y_j)^2
\end{equation*}
This is an example of a \dfn{loss function}, which is simply something we want to minimize to solve our inference problem. 
This means that the derivative of $L$ with respect to each $\beta_i$ is zero. 
\begin{align*}
\frac{\partial}{\partial \beta_i}L(\beta) &=  \sum_{z=1}^N2(\hat{y}(X_z,D)- Y_z)\frac{\partial}{\partial \beta_i}\hat{y}(X_j,D)\\
&=  \sum_{z=1}^N2(\hat{y}(X_z,D)- Y_z)\phi_i(X_z)\\
&= 2\sum_{z=1}^N\sum_{j=1}^{K} \beta_j\phi_i(X_z)\phi_j(X_z)- 2\sum_{z=1}^NY_z\phi_i(X_z)\\
&= 2\sum_{z=1}^{K} \beta_j\sum_{z=1}^N \phi_i(X_z)\phi_j(X_z)- 2\sum_{z=1}^NY_z\phi_i(X_z)\\
&=2\sum_{j=1}^{K}\beta_j \Omega_{i,j} - 2\sum_{z=1}^NY_z\phi_i(X_z)
\end{align*}
Setting 
\begin{equation*}
\frac{\partial}{\partial \beta_i}L(\beta) =0 
\end{equation*}
leads to Equation \ref{eq:barbeta} without the prior term. 
\item The regularization view of Equation \ref{eq:barbeta} is that, instead of thinking of the additional term as coming from priors, we think about adding a term to our loss function which penalizes models with too much flexibility. A common choice is   
\begin{equation*}
L(\beta) = \sum_{j=1}^N(\hat{y}(X_j,D)- Y_j)^2 + \lambda \sum_{j=1}^K \beta_j^2
\end{equation*}
This is called \dfn{ridge regression} and $\lambda$ is a parameter controlling how large a penalty we place on large values of $\beta_j$. 
If you compute the partial derivatives and equate it to zero, you will find that the equations $\beta_j$ satisfy are exactly Equation \ref{eq:barbeta} with $\sigma/\tau_i = \sqrt{\lambda}$. More generally, we could use the loss function 
\begin{equation*}
L(\beta) = \sum_{j=1}^N(\hat{y}(X_j,D)- Y_j)^2 +  \sum_{j=1}^K\left( \frac{\sigma}{\tau_i}\right)^2\beta_j^2
\end{equation*}
we would obtain exactly Equation \ref{eq:barbeta}.
\end{itemize}



\newpage

\begin{exercise}[Bayesian linear regression for Bernoulli model]
 Suppose you are doing a poll of Dartmouth students political party affiliation and for simplicity suppose there are only two options, democrat or republican.  
\begin{enumerate}[label=(\alph*)]
\item Select a prior distribution for $q$ is in Example \ref{ex:beta2}. You will need to use Equations \ref{eq:betameanvar} (but you don't need to memorize these). This step is subjective and the goal is to see how your own prior knowledge interacts with the data. Plot the prior distribution in Python. 
\item Now assume $N$ students are surveyed. They are all democrats. Compute and plot the posterior distribution for different values of $N$ on the same plot as the prior distribution. Also plot the posterior mean and standard deviation as a function of $N$.
\item How large does $N$ need to be in order for you to be $95\%$ confident that $95\%$ of Dartmouth students are democrats? You can either estimate this by hand (using Equations \ref{eq:betameanvar} again) or with simulations (by sampling the posterior). 
\end{enumerate}
\end{exercise}

\begin{exercise}[Normal model and MSE]
Consider the Bayesian Normal model with known $\sigma$. Suppose that the true value of $\mu$ is $\mu_0$. Bayesian inference leads to the posterior mean 
\begin{equation}
\hat{\mu}_P = E[\mu|\bar{X}] =  m(1-b_1) +\bar{X} b_1= m(1-b_1) +\hat{\mu}b_1
\end{equation}
which can be seen as an estimator of $\mu$. 

In class we calculated the MSE in the usual sense, meaning 
\begin{equation}
{\rm MSE}_{\hat{\mu}_P} = E[(\hat{\mu}_P- \mu_0)^2]
\end{equation}
where the expectation is taken over the distribution of our data. 

An alternative way to defined MSE is as the expected squared difference between the true value and a sample $\mu$ from the posterior averaged over all $\bar{X}$; that is
\begin{equation}
{\rm MSE}_{\hat{\mu}_P}' = E[E[(\mu- \mu_0)^2|\bar{X}]]. 
\end{equation}
In particular, the first expectation is over $\mu|\bar{X}$ ($\mu_0$ is fixed) and the second is over the data distribution. 
Compute this quantity and interpret the terms. Compare the bias and variance to what we found for the other case. 

\end{exercise}


\begin{exercise}[Bayesian linear regression \ding{111}]

\begin{enumerate}[label=(\alph*)]
\item Finish the calculation leading to Eq. \ref{eq:bayes_lr_posterior}. 
\item Generalize Eq. \ref{eq:bayes_lr_posterior} to the case when the posterior mean is not zero \end{enumerate}
\end{exercise}



\begin{exercise}[Normal model with unknown variance \ding{111} ]
Consider a Bayesian model of a Normal distribution with unknown variance. In this case we place priors on $\sigma^2$ as well, but we cannot use a Normal distribution since $\sigma$ needs to be positive. A common approach is to take $\ln \sigma$ be Normal,  leading to the Bayesian model 
\begin{align}
\ln \sigma &\sim {\rm Normal}(l,\gamma^2)\\
\mu &\sim {\rm Normal}(m,s^2)\\
\bar{X}|\mu,\sigma &\sim {\rm Normal}(\mu,\sigma^2)
\end{align}
where $\ln \sigma$ and $\mu$ are independent under the prior distribution.
Pick some values for $v$ and $\gamma^2$ and write code to generate samples from the marginal distribution of $\bar{X}$. Is this a Normal distribution? Compare the to a Normal density with the same mean and variance.  
\end{exercise}









 \bibliographystyle{unsrt}
\bibliography{./../refs.bib}


\end{document}



\section{The kernel trick}
\begin{itemize}
\item The problem of computing $\hat{y}(x,D)$ is an example of smoothing, or interpolating. Smoothing refers to the situation where we are given noisy measurements of a function $f(x)$ and want to ``smooth out the noise''. One way to do this is take the weighted averaging of neighboring values of $y$. This following examples illustrates how regression modeling is related to smoothing. 


\begin{example}[Orthogonal empirical covariance matrix]
Consider the special case where the empirical covariance matrix is diagonal -- that is, $\Omega_{i,j} = 1$ if $i=j$ and $0$ if $i\ne j$. Recall that this is the case when we use the fourier model and the $X_i$ are equally spaced. 


\noindent 
\underline{Question}: 
\begin{enumerate}[label=(\alph*)]
\item Write down a formula for $\hat{y}(x,D)$ in this case. In particular, show how to express $\hat{y}(x,D)$ in the form 
\begin{equation}\label{eq:hatykernel}
\hat{y}(x,D) = \sum_{j=1}^NY_j\kappa(x,X_j)
\end{equation}
for some function $\kappa(x,x')$. 
\item What does this function $\kappa(x,x')$ look like when $f(x)$ is a Fourier model?  \\
\end{enumerate}


\noindent 
\underline{Solution}:

\begin{enumerate}[label=(\alph*)]
\item In this case, 
\begin{equation*}
\bar{\beta}_i = \frac{1}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}\sum_{j=1}^N\phi_i(X_j)Y_j
\end{equation*}
and hence 
\begin{align*}
\hat{y}(x,D) &=  \sum_{i=1}^K\frac{\phi_i(x)}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}\sum_{j=1}^N\phi_i(X_j)Y_j\\
&= \sum_{j=1}^NY_j\left( \sum_{i=1}^K\frac{\phi_i(x)}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}\phi_i(X_j)\right)
\end{align*}
so 
\begin{equation*}
\kappa(x,x') = \sum_{i=1}^K\frac{\phi_i(x)\phi_i(x')}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}
\end{equation*}

\item For the Fourier model  with $L=1$, 
\begin{equation*}
\kappa(x,x') = \sum_{i=1}^K\frac{\sin(2\pi i x)\sin(2\pi i x')+ \cos(2\pi i x)\cos(2\pi i x')}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}
\end{equation*}
Using the identity 
\begin{align*}
\cos(x)\cos(y) + \sin(x)\sin(y) = \cos(x-y)
\end{align*}
and the fact that $\cos(x-y) = \cos(y-x)$, we get can rewrite this as
\begin{equation*}
\kappa(x,x') = \eta(|x-x'|) =  \sum_{i=1}^K\frac{\cos(2\pi i |x-x'|)}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}
\end{equation*}
\end{enumerate}
\end{example}

\item We actually don't need the empirical covariance matrix to be diagonal to ``kernalize'' our predictions (meaning express $\hat{y}(x,D)$ in the form of Equation \ref{eq:hatykernel}. Equation \ref{eq:yhatmatrixform} also has this form, although we can not determine the kernel without inverting a very large matrix. 
%\item In light of this example, it seems that in some contexts we can achieve the same goal of regression by bypassing the entirely regression formulation and simply picking a suitable function $\kappa(x,x')$ as our Kernel. We can then make predictions by writing
%\begin{equation*}
%\hat{y}(x,D) = \sum_{i=1}^N\kappa(x,X_i)Y_i 
%\end{equation*} 
%This is called a Kernal smoother. 

\end{itemize}










%----------------------------------------------------------------------------------------------------------------
%\section{Regularization view of priors}
%\begin{itemize}
%\item There is 
%\end{itemize}


%----------------------------------------------------------------------------------------------------------------
%\section{Kernel based view}
%\begin{itemize}
%\item Suppose we have a 
%\end{itemize}


%----------------------------------------------------------------------------------------------------------------
%\section{Regularization}
%\begin{itemize}
%\item We've seen that adding flexibility to our models comes at the expense of precision, or variance in our predictions. Is there some way to build models that are very flexible, but somehow still constrained so as not to be to be very variable? One approach is known as \dfn{regularization}. This is especially important if we want to add more parameters than data points. 
%\item To understand regularization, recall that our current fitting procedure for a regression model computes $\hat{\beta}_i$ from 
%\begin{equation*}
%{\rm min}_{\beta} \sum_{i=1}^N(\hat{y}(X_i,D) - Y_i)^2
%\end{equation*}
%The idea of regularization is to 
%\end{itemize}









