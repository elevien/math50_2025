\include{./../latex/notes_style.tex}


\setcounter{unit}{1}
\setcounter{section}{0}


\begin{document}



\title{Unit 6: regularization, priors and Bayesian inference}
\author{Ethan Levien}
\maketitle


\section{Introduction to Bayesian inference: some simple examples}

So far, we think of models encode our assumptions about how data was generate. Our models depend on parameters for which the true values are fixed. When use data to \dfn{fit} or \dfn{infer} parameters, obtain a sample distribution which roughly speaking quantifies uncertainty in our estimates. This way of performing statistics, where the uncertainty is thought of as a distribution over repeated experiments, is \dfn{frequentist} approach to statistics.  In this formulation of statistical inference we are pretending to have complete ignorance of the parameters before we see the data, but in reality, this is never true. For example, if we flip a coin $N=3$ times and get $Y=3$ heads, our estimate of the probability this coin will land on heads in the next flip is $\hat{q} = 1$, which is clearly not in line with out understanding that both sides are at least possible. It is also worth noting that the usual estimate standard error, ${\rm se}(\hat{q}) = \sqrt{q(1-q)/N} \approx  \sqrt{\hat{q} (1-\hat{q} )/N}$ gives zero, but this is obviously not a good estimate of the uncertainty. 

Similar issues emerge in the context of more machine learning-style data analysis, sometimes we want to incorporate vague information, such as ``the function describing my data is very smooth and changes roughly on a time-scale of 5 hours". Or, we might want to include many predictors, but to avoid overfitting penalize high values of the predictors. For example, we might believe there is an interaction term, but suspect it is much smaller than the additive terms. 

 While there are ways handle these problems within the frequentist framework, they are handled more naturally by taking an entirely different treatment of parameters and statistical inference. This is called \dfn{Bayesian statistics}. In the Bayesian formulation of statistics, we will think of parameters as themselves random variables which are given a distribution before we have seen the data. Mathematically, this means instead of having a model with fixed parameter: 
 \begin{equation}
X \sim {\rm ModelDistribution}(\theta)
 \end{equation}
 we think of our model as a conditional on $\theta$
  \begin{equation}
X|\theta \sim {\rm ModelDistribution}(\theta). 
 \end{equation}
We call $X|\theta$ the \dfn{likelihood} (this term is used in both frequentists and Bayesian approaches). 
Then, we add a new distribution for $\theta$, called the prior. The goal is then to condition on the observed data $X_1,\dots,X_N$ to find a new distribution $\theta|X_1,\dots,X_N$, called the posterior. The posterior plays a similar role to the sample distribution, and naturally we often use its mean as an estimate of $\theta$. However, there are both philosophical and mathematical differences between these two approaches which make the comparison imperfect. We will focus less on the philosophical aspect and see what is practically different about the approaches. This is best illustrated with the following example. 
 
 

\subsection{Bayesian inference for Bernoulli trails}

Consider the model 
\begin{equation*}
X \sim {\rm Bernoullli}(q)
\end{equation*}
for the outcome of a coin flip. If we perform $N$ flips, then recall $Y = \sum_{i=1}^NX_i$ is binomial. 
In a Bayesian model, we would specify the prior distribution of $q$. This should reflect what we know about the coin, which in reality is that it is very likely to be fair. 
However, to make calculations simpler and better compare the frequentist approach we will take a prior where any value of $q$ is equally likely. This seems to be more similar to what we are doing in the frequentist approach. Our Bayesian model would then me
\begin{align*}
q &\sim {\rm Uniform}(0,1)\\
X|q &\sim {\rm Bernoullli}(q)
\end{align*}
%Another way to think about constructing an estimator of $q$, is that given data points $X_1,\dots,X_N$, we would like to know 
%\begin{equation}
%\hat{q}_b = E[q|X_1,\dots,X_N]\label{eq:qhatb}
%\end{equation}
%We can in principle solve this problem using Bayes' theorem. In fact, we would also take it a step further and ask: What is the distribution of
Our goal is to find the posterior distribution 
\begin{equation*}
q|X_1,\dots,X_N
\end{equation*}
Since the flips out independent, this distribution depends only on $Y$, so we can alternative obtain the posterior as the distribution of $q|Y$. 
Recall that the density of the uniform is simply $f(q) = 1$ (I'll use $f$ for a density and $P$ for probabilities, but sometimes we have to mix them). Using Bayes' theorem (or simply using the definition of conditional probability twice), we have 
\begin{equation}
f(q|Y=y) = \frac{f(q,\{Y=y\})}{P(Y=y)} = \frac{P(Y=y|q)f(q)}{P(Y=y)} 
\end{equation}
where $f(q,\{Y=y\})$ means the joint density/probability of $q$ and $Y$. 
Recall the binomial distribution is 
\begin{equation}
P(Y=y|q) = {N \choose y}q^y(1-q)^{N-y}
\end{equation}
The important thing to realize is that all the $q$ dependence comes from $P(Y=y|q)$ and since our goal is to find a distribution of $q$, we can write
\begin{equation}
f(q|Y=y) = C(N,y)q^y(1-q)^{N-y}
\end{equation}
where $C(N,y)$ is a constant which ensures $\int f(q|Y=y)dq = 1$. That is, 
\begin{equation}
C(N,y)^{-1} = \int_0^1 q^y(1-q)^{N-y} dq
\end{equation} 
$f(q|Y=y)$ is an example of \dfn{Beta distribution}. More generally, we say a variable $q$ is
\begin{equation}
q \sim {\rm Beta}(a,b)
\end{equation}
if $q$ has the density 
\begin{equation}
f(q) = q^{a-1}(1-q)^{b-1}
\end{equation}
hence $q|Y \sim {\rm Beta}(y+1,N-y+1)$. 
 You don't need to memorize this, but you should look on Wikipedia or in the colab notebook to get a sense of what this function looks like. 




\begin{example}[Laplace Rule of Succession of Bayesian view]\label{ex:beta}

Consider Bayesian inference for Bernoulli trials with uniform priors as described above. Recall that Laplace's Rule of Succession gives the estimator 
\begin{equation}
\hat{q}_L = \frac{Y+1}{N+2}
\end{equation}


\noindent
\underline{Question}: Show that when $Y=N$
\begin{equation*}
E[q|Y] = \frac{N+1}{N+2} = \hat{q}_L
\end{equation*}

\noindent
\underline{Solution}: 
In this case 
\begin{equation}
C(N,y)^{-1} =C(N,N)^{-1} = \int_0^1 q^N dq = \frac{1}{N+1}
\end{equation} 
and 
\begin{equation}
E[q|Y]  = (N+1) \int_0^1 q \times q^{N}dq = (N+1) \int_0^1 q^{N+1}dq  = \frac{N+1}{N+2}. 
\end{equation}


%It turns out that this has a relatively simple form, called a $\beta$-distribution. We say 
%\begin{equation*}
%q \sim {\rm Beta}(a,b)
%\end{equation*}
%if $q$ is a random variable on $[0,1]$ which has density 
%\begin{equation*}
%f(q) = Bq^{a-1}(1-q)^{b-1}
%\end{equation*}
%The constant $B$ ensures the area under this curve between $q=0$ and $q=1$ is indeed one, as it must be for a random density. \\
%



%\noindent
%\underline{Solution}: Using Bayes' theorem 
%\begin{align*}
%f(q|Y) &= \frac{P(Y|q)f(q)}{P(Y)}\\
%&=  \frac{1}{P(Y)}{N \choose Y}q^{Y}(1-q)^{N-Y}
%\end{align*}
%Since we are thinking of this as a probability density in $q$, we don't actually need to compute $P(Y)$ explicitly (even though we could), instead, we can simply notice that 
%\begin{equation*}
%f(q|Y)  = Cq^{Y}(1-q)^{N-Y}
%\end{equation*}
%for some constant of proportionality $C$ which will depend on $Y$, but is uniquely determined by the fact that the area under this curve must be $1$.  In the colab notebook we plot this curve. 
\end{example}


 To summarize the idea of Bayesian inference, If $Y$ is a our data and $\theta$ is our parameter(s) Bayes' theorem tells us that 
\begin{equation}\label{eq:bayes}
P(\theta|Y) = \frac{P(Y|\theta)P(\theta)}{P(Y)}
\end{equation}



 The distributions appearing in Equation \eqref{eq:bayes} have the following names and interpretations:
\begin{itemize}
\item $P(\theta|Y)$ is the \dfn{posterior} (a beta distribution in Example \ref{ex:beta}).
\item $P(Y|\theta)$ is the \dfn{likelihood} (Binomial distribution in Example \ref{ex:beta}).
\item $P(\theta)$ is the prior distribution (Uniform distribution in Example \ref{ex:beta})
\item $P(Y)$ is the evidence. It represents the chance we observe the data \emph{unconditional} on the parameters. This can be obtained by marginalizing over the priors.
\end{itemize}


While in classical statistics, the objective is to determine (estimate) a parameter $\theta$ and measure its uncertainty (with the sample distribution), in Bayesian statistics our objective is to compute the posteior distribution. Once we have this, we can obtain so-called \dfn {point estimates}, for example, by taking the average of $\theta$ or maximum of $P(\theta|Y)$ (maximum likelihood).  The point estimates are however less central in Bayesian inference than the posterior. 

\subsection{Selecting priors with a beta distribution (optional)}
 The beta distribution will play an important role in what follows, so we note that if 
\begin{equation*}
q \sim {\rm Beta}(a,b)
\end{equation*}
then (using calculus) it can be shown that 
\begin{align*}
E[q] &= \frac{a}{a+b}\\
{\rm var}(q) &= \frac{ab}{(a+b)^2(a+b+1)}.
\end{align*}
Notice that this implies the estimator $\hat{q}_b$ in Equation \ref{eq:qhatb} becomes 
\begin{equation*}
\hat{q}_b = \frac{Y+1}{Y+1+N-Y+1} = \frac{Y+1}{N+2}
\end{equation*}
This is in-fact the estimator $\hat{q}_L$ from the week 7 exercise set!

 The \dfn{mode} -- the value which maximizes the probability density -- for the $\beta$-distribution is 
\begin{equation*}
\frac{a-1}{a+b-2}
\end{equation*}
 We can see that  the estimator $\hat{q} = Y/N$ comes comes from the mode $f(q|Y)$ -- recall that such an estimator is called the maximum likelihood estimator.   Adding priors usually has the effect of introducing a bias while reducing the variance. 
%Another important feature of the $\beta$ distribution is that when $a$ and $b$ are very large, it is approximately Normal with this mean and variance. This means we can get a rough idea of the probabilities for a $\beta$ random variable. Equipped with this knowledge, we can try to go beyond the case where our prior assumption about the distribution of $q$, before fitting the model, is Uniform. 

\begin{example}[Selecting priors]\label{ex:beta2}
Suppose we are given a coin. We are trying to determine whether it is biased based on the outcome of $N$ flips. We are pretty sure that, like most coins, it is not biased. To be precise, let's say you are $95\%$ confident that the bias of the coin is less than $10\%$ biased towards heads or tails.\\

\noindent 
\underline{Question}: 
\begin{enumerate}[label=(\alph*)]
\item Select a prior distribution for $q$. 
\item What is the posterior expectation if we flip the coin $5$ times and see $5$ heads? \\
\end{enumerate} 


\noindent 
\underline{Solution}: 
\begin{enumerate}[label=(\alph*)]
\item We know that a $\beta$-distribution is approximately Normal if $a$ and $b$ are large enough. Assuming we can make a Normal approximation, we can use the formulas for the mean and variance of the $b$ distribution to select parameters such that 
\begin{equation}\label{eq:Pqpriors}
P(|q-0.5|<0.1)  = P(0.4<q<0.6) \approx 0.95
\end{equation}
Since we would like the mean of our prior distribution to be $1/2$, we select
\begin{equation*}
q \sim {\rm Beta}(a,a).
\end{equation*}
If $q$ is approximately Normal, then Equation \ref{eq:Pqpriors} will hold when 
\begin{align*}
1.96 \sqrt{{\rm var}(q)} &= 0.1 \\
&\implies  \sqrt{\frac{a^2}{4a^2(2a+1)}} = \frac{1}{2}\sqrt{\frac{1}{2a+1}}= \frac{0.1}{1.6}\\
&\implies a \approx 31.5
\end{align*}
\item In order to compute the posterior, we use Bayes' theorem 
\begin{equation*}
f(q|X_1,\dots,X_N) \propto q^{Y}(1-q)^{N-Y} \times q^{a-1}(1-q)^{a-1} = q^{Y+a-1}(1-q)^{N-Y+a-1}
\end{equation*}
where $Y = \sum_{i=1}^NX_i$. 
Again, the $q$ dependence uniquely determines the distribution, since the area under this curve must be one. We can therefore say that 
\begin{equation*}
q|X_1,\dots,X_N \sim {\rm Beta}(Y+a,N-Y+a)
\end{equation*}
For the mean and variance we have
\begin{align*}
E[q|X_1,\dots,X_N] &= \frac{Y+a}{N+2a} =  \frac{5+31.5}{5+2\times 31.5}  \approx 0.54
\end{align*}
Note that if we used usual estimator we would have found $\hat{q} = 1$, while if we used uniform priors and taken the posterior expectation $\hat{q}_L = 0.85$. Yet another approach would be to take the posterior maximum (the mode) of the posterior in using the $\beta$-distribution priors. 

\end{enumerate}

\end{example}

% Something quite nice about using $\beta$-distribution priors for the Bernoulli model is that BOTH the prior and the posterior have a $\beta$-distribution, albeit with difference parameters. In general, when this is the case we say that the priors are \dfn{conjugate}. 

\subsection{Bayesian inference for Normal mean}

Suppose we have a Normal model for $Y$, but treat the noise variance as a constant and focus on Bayesian inference of $\mu$. 
\begin{equation*}
Y|\mu \sim {\rm Normal}(\mu,\sigma^2/N).
\end{equation*}
It is natural to take Normal priors on $\mu$, since then the marginal of $Y$ is Normal (you should check this!) making our calculations much easier. Hence we set 
\begin{equation*}
\mu \sim {\rm Normal}(m,s^2)
\end{equation*}
Notice that this is nothing but a linear regression model for $Y$ with $\mu$ as a predictor (and regression slope $\beta =1$). Therefore, the computation of the posterior $\mu|Y$ amounts to ``inverting'' a linear regression model, something we are familiar with from a number of examples and exercises in previous units. 

In particular, since $\mu|Y$ is Normal, we only need to calculate the mean and variance.  
\begin{align}
m &= E[\mu] =  b_1 E[Y] + b_0\\
s^2 &= {\rm var}(\mu) =b_1^2 {\rm var}(Y) + {\rm var}(\mu|Y)
\end{align}
where $b_1$ and $b_0$ are the regression slope and intercept when $Y$ is treated as the predictor. In particular, $b_0 = E[\mu|Y=0]$. 

The variance and expected value of $Y$ are
\begin{equation}
E[Y] = m,\quad {\rm var}(Y) = s^2 + \sigma^2/N. 
\end{equation} 
The regression slope $b$ appearing in the above formula can be found from the covariance. Using the relation ${\rm cov}(X,Y) = \beta_1 \sigma_X^2/Users/elevien/Dartmouth College Dropbox/Ethan Levien/TEACHING/Dartmouth/math50_2024/course_documents_2024/Math50-w8W_CLASS.pdf$ (here $X$ is $\mu$ and $\beta_1=1$), we have
\begin{equation}
{\rm cov}(\mu,Y) = s^2
\end{equation}
hence the ``inverse'' regression slope is 
\begin{equation}
{\rm cov}(\mu,Y) = b_1(s^2 + \sigma^2/N) \implies b_1= \frac{1}{1 + \left(\frac{\sigma}{s}\right)^2\frac{1}{N}}
\end{equation}
Coming these facts yields 
\begin{equation}
b_0 = m\left(1 -   \frac{1}{1 + \left(\frac{\sigma}{s}\right)^2\frac{1}{N}}\right) = m \frac{ \left(\frac{\sigma}{s}\right)^2\frac{1}{N}}{1 + \left(\frac{\sigma}{s}\right)^2\frac{1}{N}}.
\end{equation}
Finally 
\begin{align*}
{\rm var}(\mu|Y) &= \frac{s^2}{N}  - b_1^2 {\rm var}(Y) = \frac{s^2}{N} -  \frac{ s^2 + \sigma^2/N}{(1 + \left(\frac{\sigma}{s}\right)^2\frac{1}{N})^2}\\
&\quad \vdots \quad (\text{some algebra})\\
&= \frac{s^2\sigma^2}{s^2N + \sigma^2}
\end{align*}
In summary, the posterior distribution 
\begin{equation}
\mu|Y \sim {\rm Normal}\left(b_1Y + (1-b_1), \frac{s^2\sigma^2}{s^2N + \sigma^2}\right).
\end{equation}
Take note of what happens when $\sigma/s$ is very large/small. 


 




%----------------------------------------------------------------------------------------------------------------
%\section{Bayesian inference for regression with single predictor}
%\subsection{An analytically tractable example}
%\begin{itemize}
%\item In the general, the posterior is difficult or impossible to find a formula for. Instead, we need to relay on numerical methods. However, for the linear regression it turns out we can select the priors so that the posterior is analytically tractable, must like we did  
%To see how this works, we start with the inference for a Normal distribution
%\begin{equation}
%Y \sim {\rm Normal}(\mu,\sigma)
%\end{equation}
% (we can think of this as a linear regression model with $a = 0$ and $\mu = b$). 
% Recall that the distribution of the data $D = \{(Y_1,X_1),\dots,(Y_N,X_N)\}$ is 
%\begin{align}
%p(D|\theta) &= \prod_i \frac{1}{\sqrt{2 \sigma^2 \pi}}e^{-(Y_i - \mu)^2/2\sigma^2}\\
%&=  \frac{1}{(2 \sigma^2 \pi)^{N/2}}e^{-\sum_i (Y_i - \mu)^2/2\sigma^2}
%\end{align}
%We can notice that if the priors are very flat, then this is also proportional to the posterior distribution. Let's start by pretending that $\sigma_{\epsilon}^2$ is known and take our priors on $\mu$ to be Normal as well
%\begin{equation}
%\mu \sim {\rm Normal}(\mu_0,\sigma_{\mu})
%\end{equation}
%Then the posterior is \emph{proportional to} 
%\begin{align}\label{eq:post_normal}
% p(D|\theta)p(\theta) &= \frac{1}{(2 \sigma^2 \pi)^{N/2}}e^{-\sum_i (Y_i - \mu)^2/2\sigma^2}\frac{1}{\sqrt{2\pi \sigma_{\mu}^2}}e^{-(\mu-\mu_0)^2/2\sigma_{\mu}^2}\\
%&=  \frac{1}{(2 \sigma^2 \pi)^{N/2}}\frac{1}{\sqrt{2\pi \sigma_{\mu}^2}}e^{-\sum_i (Y_i - \mu)^2/2\sigma^2 - (\mu-\mu_0)^2/2\sigma_{\mu}^2}
%\end{align}
%On this surface this looks a bit complicated as a function of $\mu$, but there is a trick: Notice that all the dependence on $\mu$ comes from the exponent. 
%%Let's simplify the exponent to get something that looks more recognizable. 
%%\begin{align}
%%-  \mu \left( \sum_i Y_i\right)/\sigma^2   + \mu^2/2\sigma^2  + \mu\mu_0/\sigma_{\mu}^2 - \mu^2/2\sigma_{\mu}^2
%%\end{align}
%We can rewrite this as 
%\begin{equation}
%A\mu^2 + B \mu  + C 
%\end{equation}
%where
%\begin{align}
%A &= \frac{1}{2}\left(\frac{N}{\sigma^2} + \frac{1}{\sigma_{\mu}^2}\right)\\
%B &= -\frac{\sum_i Y_i}{\sigma^2} - \frac{\mu_0}{\sigma_{\mu}^2}\\
%C &= \frac{\sum_i Y_i^2}{2\sigma^2} + \frac{\mu_0^2}{\sigma_{\mu}^2}
%\end{align}
%If we factor this quadratic equation, we find that is can be written 
%\begin{equation}
%A(\mu - B/2A)^2 + {\rm const.}
%\end{equation} 
%We don't care what the constant terms is, since the it is the first term which tells us the mean and standard deviation of $\mu$. Now observe that 
%\begin{equation}
%-\frac{B}{2A} = \bar{Y} \frac{\sigma_{\mu}^2}{\sigma_{\mu}^2 + \sigma^2/N} + \mu_0 \frac{\sigma^2/N}{\sigma_{\mu}^2 + \sigma^2/N}  \equiv \mu_b
%\end{equation}
%and 
%\begin{equation}
%\frac{1}{A} =2\frac{\sigma_{\mu}^2\sigma^2/N}{\sigma_{\mu}^2+\sigma^2/N} \equiv 2\sigma_b^2
%\end{equation}
%In particular, we can deduce that 
%\begin{equation}
%\mu|D \sim {\rm Normal}\left( \mu_b,\sigma_b \right)
%\end{equation}
%where $\mu_b$ and $\sigma_b^2$ defined above are the mean and variance of the posterior distribution. Despite all the messiness, these actually have very clear interpretations which will give us an intuition above how Bayesian statistics really works. To understand these, think about the following questions: 
%\begin{enumerate}
%\item What happens as $N \to \infty$? 
%\item What happens as $\sigma^2 \to 0$ or $\infty$? 
%\item What happens as $\sigma_{\mu} \to 0$ or $\infty$? 
%\end{enumerate}
%\end{itemize}



%----------------------------------------------------------------------------------------------------------------
\section{Bayesian inference for linear regression models}
  We now discuss Bayesian inference for the general linear regression model with features. Let
\begin{equation*}
f(x) = \sum_{i=1}^K\beta_i\phi_i(x)
\end{equation*}
and suppose that our priors are 
\begin{equation*}
\beta_i \sim {\rm Normal}(0,\tau_i^2).
\end{equation*}
We will assume that $E[\phi_i(X)]=0$. We will also assume, for simplicity, that {\bf $\sigma$ is known. Hence, we do not need to consider estimating it.} This simplifies the conceptual picture considerably. 

 In this case, the Posterior distribution of the $\beta_i$ can be understood analytically. It turns out that the marginal posterior distribution of each $\beta_i$ is again Normal -- thus Normal priors on $\beta_i$ are conjugate priors!. The marginal mean of each $\beta_i$ are determined by the system of questions given in the following Theorem. 


\begin{thm}[Posterior mean for regression coefficients]\label{thm:regpostmean}
Define a $K\times K$ matrix $\Omega$ called the \dfn{empirical covariance matrix} which has entries
\begin{equation*}
\Omega_{i,j} = N\overline{\phi_i(X)\phi_j(X)} = \sum_{k=1}^N\sum_{z=1}^N\phi_i(X_k)\phi_j(X_z) 
\end{equation*}
and let $\bar{\beta}_i$ denote the posterior mean of $\beta_i$; that is, 
\begin{equation*}
\bar{\beta}_i =  E[\beta_i|D]. 
\end{equation*}
Then $\bar{\beta}_1,\dots,\bar{\beta}_K$ satisfy the $K$ equations 
\begin{equation}\label{eq:barbeta}
 \left(\frac{\sigma}{\tau_i}\right)^{2}\bar{\beta}_i  +\sum_{j=1}^K\Omega_{i,j}\bar{\beta}_j = \sum_{j=1}^N\phi_i(X_j)Y_j
\end{equation}

\end{thm}
We make a few remarks on this Theorem:
\begin{itemize}
\item If $N> K$, then $\bar{\beta}_i$ actually have a solution. In-fact, depending on $\Omega$, there may not be a solution. For our discussion, we will assume there is however. 
\item Under the assumption that $E[\phi_i(X)]=0$, the entries of $\Omega_{i,j}$ approximate the covariances between the predictor features. Moreover, if we take the predictors to be drawn from some distribution and average over the data (both $X$ and $Y$), we get
\begin{equation*}
\frac{E[\Omega_{i,j}]}{N}=  {\rm cov}(\phi_j(X),\phi_i(X)).
\end{equation*}
and  
\begin{equation*}
\frac{1}{N}\sum_{j=1}^NE[\phi_i(X_j)Y_j]= {\rm cov}(Y,\phi_i(X)).
\end{equation*}
Hence, we have the ``math world'' version of Equations \ref{eq:barbeta}, which is obtained by averaging over the data: 
\begin{equation*}
 \frac{1}{N}\left(\frac{\sigma}{\tau_i}\right)^{2}E[\bar{\beta}_i]  + \sum_{j=1}^K{\rm cov}(\phi_i(X),\phi_j(X))E[\bar{\beta}_j] = {\rm cov}(Y,\phi_i(X)).
\end{equation*}
In the limit $N \to \infty$ and with $K=2$, we obtain a system of equations recognizable from Week 5 notes for the regression coefficients in the two predictor model in terms of the covariances. In this context, the role of the regression coefficients (which we previously took to be fixed numbers), is now played by the averages $E[\bar{\beta}_j]$, which are the average values of $\beta_j$ with respect to both the posterior and the data. 
\item Notice that the first term in  Equation \ref{eq:barbeta} vanishes as either $\tau_i \to \infty$ (very broad priors) or $\sigma \to 0$ (no variance in $Y$ conditional on the predictors). In both cases, the vales of $\bar{\beta}_i$ are entirely determined by the data, and the priors play no role. When $\tau_i \to 0$ or $\sigma \to \infty$, the priors entirely determine determine $\bar{\beta}_i$ and in fact $\bar{\beta}_i =0$. 
\item The first term in  Equation \ref{eq:barbeta} is an example \dfn{regularization}. 
\end{itemize}


 Theorem \ref{thm:regpostmean} can be elegantly stated using matrices (see Linear algebra review note for intro to matrix multiplication).  This is import if we want to implement these computations in Python. Observe that one way to construct $\Omega_{i,j}$ is to define the matrices
\begin{align*}
A &= \left[ 
\begin{array}{cccc}
\vertbar & \vertbar& & \vertbar\\
\phi_1(X) & \phi_2(X) & \cdots & \phi_K(X) \\
\vertbar & \vertbar& & \vertbar
\end{array}
\right],\quad 
\Lambda_0 = \left[
\begin{array}{cccc}
\tau_1 & 0 & \dots & 0 \\
0 & \tau_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \tau_K \\
\end{array}
\right]
\end{align*}
Then $\Omega = A^TA$. 
Now define another matrix $K\times K$ matrix 
\begin{equation*}
M = \Omega + \sigma^2\Lambda_0^{-1}
\end{equation*}
Then Equations \ref{eq:barbeta} can be written as 
\begin{equation*}
M \bar{\beta} = A^Ty
\end{equation*}
where $\bar{\beta}$ and $y$ are respectively a $K$-vector and $N$-vector
\begin{equation*}
\bar{\beta} = \left[\begin{array}{c} \bar{\beta}_1\\ 
\vdots\\
\bar{\beta}_K \end{array}\right],\quad y = \left[\begin{array}{c}Y_1\\ 
\vdots\\
Y_N \end{array}\right]. 
\end{equation*}
Then $\bar{\beta}$ satisfies  
\begin{equation*}
M\bar{\beta} = A^Ty  \implies \bar{\beta} =  \left( A^TA+ \sigma^2\Lambda_0^{-1}\right)^{-1}A^Ty
\end{equation*}



\begin{example}[Calculating posterior mean in python]\label{ex:regpostmean}

Consider the model with two predictors:
\begin{equation*}
f(x) = \beta_1X_1 + \beta_2X_2\\
\end{equation*}


\noindent 
\underline{Question}: Generate some simulated data and compute the posterior means of $\beta$ for different choices of $\tau$. \\


\noindent 
\underline{Solution}: See  \href{https://colab.research.google.com/drive/1sFlY0nvo7hrsNYGRdx8C4tKPihyOSmbx?usp=sharing}{colab notebook}. 

\end{example}




\subsection{Posterior prediction}


 We will mostly focus on how we can use the posterior estimates to make predictions, although there is an entirely different set of questions we could ask concerning model assessment and hypothesis testing.Recall that $\hat{y}(x,D)$ is defined as our prediction, using the regression model fitted to the data $D$, of $E[Y|X=x]$. In the Bayesian context, we can define the posterior predictive variable $Y|X,D$ as the random representing the distribution of our response variable conditioned on the data. We can obtain this distribution by replacing the $\beta_i$s in 
\begin{equation*}
Y = \sum_{i=1}^{K}\beta_i\phi_i(X)
\end{equation*}
with samples from the posterior distribution. In the Bayesian framework we set
\begin{equation*}
\hat{y}(x,D) = E[Y|D,X] = \sum_{i=1}^{K}E[\beta_i|D]\phi_i(X) = \sum_{i=1}^{K}\bar{\beta}_i\phi_i(X) 
\end{equation*}
We can write this in matrix notation by defining the vector 
\begin{equation*}
a = \left[\begin{array}{c}
\phi_1(X)\\
\vdots\\
\phi_K(X)
\end{array}\right].
\end{equation*}
Then 
\begin{equation}\label{eq:yhatmatrixform}
\hat{y}(x,D) = a^T\bar{\beta} = a^TM^{-1}A^Ty
\end{equation}


\begin{example}[Posterior predictions for a regression model]\label{ex:fourier regularization}
Take the Fourier model
\begin{equation*}
f(x) =\beta_0 +  \sum_{i=1}^K\beta_i \sin\left(\frac{2\pi i x}{L} \right) + \alpha_i \cos\left(\frac{2\pi i x}{L} \right).
\end{equation*}
Implement a function to compute posterior predictions with different priors $\tau$ on the coefficients. In this example, we will fit this model to data generated by adding noise to the function 
\begin{equation*}
f_{\rm true}(x) = \sin(2\pi 3x) + 5\sin(2\pi 6x^2) + 20x^2
\end{equation*}
with $x \in [0,1]$.
Note that for $K<\infty$, we can not pick any values of the $\beta_i$s and $\alpha_i$s such that $f = f_{\rm true}$.  \\ 

We will assume that $\beta_i$ and $\alpha_i$ have the same prior distribution: 
\begin{align*}
\beta_i \sim {\rm Normal}(0,\tau_i)\\
\alpha_i \sim {\rm Normal}(0,\tau_i)\\
\end{align*}

\noindent 
\underline{Question}: 
Compute the posterior mean $\hat{y}(x,D)$ for the following choices of priors and $K=50$. 
\begin{enumerate}[label=(\alph*)]
\item All $\tau_i$ are the same, $\tau_i = \tau_0$. 
\item The $\tau_i$ decay with $i$ as $\tau_i = \tau_0/i$
\end{enumerate}
In each case, experiment with different choices of $\tau_0$ and pay attention to whether the model is overfitting (high variance, low bias) or under fitting (low variance, high bias). \\


\noindent 
\underline{Solution}: See  \href{https://colab.research.google.com/drive/1sFlY0nvo7hrsNYGRdx8C4tKPihyOSmbx?usp=sharing}{colab notebook}. 


 
\end{example}




%------------------------------------------------------------------------------------------------------------------------
\section{Regularization view of priors}
\begin{itemize}
\item Recall that in standard least squares linear regression, the regression coefficients come from minimizing the sum of squared residuals. That is, they are the values that minimize the function 
\begin{equation*}
L(\beta) = \sum_{j=1}^N(\hat{y}(X_j,D)- Y_j)^2
\end{equation*}
This is an example of a \dfn{loss function}, which is simply something we want to minimize to solve our inference problem. 
This means that the derivative of $L$ with respect to each $\beta_i$ is zero. 
\begin{align*}
\frac{\partial}{\partial \beta_i}L(\beta) &=  \sum_{z=1}^N2(\hat{y}(X_z,D)- Y_z)\frac{\partial}{\partial \beta_i}\hat{y}(X_j,D)\\
&=  \sum_{z=1}^N2(\hat{y}(X_z,D)- Y_z)\phi_i(X_z)\\
&= 2\sum_{z=1}^N\sum_{j=1}^{K} \beta_j\phi_i(X_z)\phi_j(X_z)- 2\sum_{z=1}^NY_z\phi_i(X_z)\\
&= 2\sum_{z=1}^{K} \beta_j\sum_{z=1}^N \phi_i(X_z)\phi_j(X_z)- 2\sum_{z=1}^NY_z\phi_i(X_z)\\
&=2\sum_{j=1}^{K}\beta_j \Omega_{i,j} - 2\sum_{z=1}^NY_z\phi_i(X_z)
\end{align*}
Setting 
\begin{equation*}
\frac{\partial}{\partial \beta_i}L(\beta) =0 
\end{equation*}
leads to Equation \ref{eq:barbeta} without the prior term. 
\item The regularization view of Equation \ref{eq:barbeta} is that, instead of thinking of the additional term as coming from priors, we think about adding a term to our loss function which penalizes models with too much flexibility. A common choice is   
\begin{equation*}
L(\beta) = \sum_{j=1}^N(\hat{y}(X_j,D)- Y_j)^2 + \lambda \sum_{j=1}^K \beta_j^2
\end{equation*}
This is called \dfn{ridge regression} and $\lambda$ is a parameter controlling how large a penalty we place on large values of $\beta_j$. 
If you compute the partial derivatives and equate it to zero, you will find that the equations $\beta_j$ satisfy are exactly Equation \ref{eq:barbeta} with $\sigma/\tau_i = \sqrt{\lambda}$. More generally, we could use the loss function 
\begin{equation*}
L(\beta) = \sum_{j=1}^N(\hat{y}(X_j,D)- Y_j)^2 +  \sum_{j=1}^K\left( \frac{\sigma}{\tau_i}\right)^2\beta_j^2
\end{equation*}
we would obtain exactly Equation \ref{eq:barbeta}.
\end{itemize}

\section{The kernel trick}
\begin{itemize}
\item The problem of computing $\hat{y}(x,D)$ is an example of smoothing, or interpolating. Smoothing refers to the situation where we are given noisy measurements of a function $f(x)$ and want to ``smooth out the noise''. One way to do this is take the weighted averaging of neighboring values of $y$. This following examples illustrates how regression modeling is related to smoothing. 


\begin{example}[Orthogonal empirical covariance matrix]
Consider the special case where the empirical covariance matrix is diagonal -- that is, $\Omega_{i,j} = 1$ if $i=j$ and $0$ if $i\ne j$. Recall that this is the case when we use the fourier model and the $X_i$ are equally spaced. 


\noindent 
\underline{Question}: 
\begin{enumerate}[label=(\alph*)]
\item Write down a formula for $\hat{y}(x,D)$ in this case. In particular, show how to express $\hat{y}(x,D)$ in the form 
\begin{equation}\label{eq:hatykernel}
\hat{y}(x,D) = \sum_{j=1}^NY_j\kappa(x,X_j)
\end{equation}
for some function $\kappa(x,x')$. 
\item What does this function $\kappa(x,x')$ look like when $f(x)$ is a Fourier model?  \\
\end{enumerate}


\noindent 
\underline{Solution}:

\begin{enumerate}[label=(\alph*)]
\item In this case, 
\begin{equation*}
\bar{\beta}_i = \frac{1}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}\sum_{j=1}^N\phi_i(X_j)Y_j
\end{equation*}
and hence 
\begin{align*}
\hat{y}(x,D) &=  \sum_{i=1}^K\frac{\phi_i(x)}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}\sum_{j=1}^N\phi_i(X_j)Y_j\\
&= \sum_{j=1}^NY_j\left( \sum_{i=1}^K\frac{\phi_i(x)}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}\phi_i(X_j)\right)
\end{align*}
so 
\begin{equation*}
\kappa(x,x') = \sum_{i=1}^K\frac{\phi_i(x)\phi_i(x')}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}
\end{equation*}

\item For the Fourier model  with $L=1$, 
\begin{equation*}
\kappa(x,x') = \sum_{i=1}^K\frac{\sin(2\pi i x)\sin(2\pi i x')+ \cos(2\pi i x)\cos(2\pi i x')}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}
\end{equation*}
Using the identity 
\begin{align*}
\cos(x)\cos(y) + \sin(x)\sin(y) = \cos(x-y)
\end{align*}
and the fact that $\cos(x-y) = \cos(y-x)$, we get can rewrite this as
\begin{equation*}
\kappa(x,x') = \eta(|x-x'|) =  \sum_{i=1}^K\frac{\cos(2\pi i |x-x'|)}{1 + \left(\frac{\sigma}{\tau_i} \right)^2}
\end{equation*}
\end{enumerate}
\end{example}

\item We actually don't need the empirical covariance matrix to be diagonal to ``kernalize'' our predictions (meaning express $\hat{y}(x,D)$ in the form of Equation \ref{eq:hatykernel}. Equation \ref{eq:yhatmatrixform} also has this form, although we can not determine the kernel without inverting a very large matrix. 
%\item In light of this example, it seems that in some contexts we can achieve the same goal of regression by bypassing the entirely regression formulation and simply picking a suitable function $\kappa(x,x')$ as our Kernel. We can then make predictions by writing
%\begin{equation*}
%\hat{y}(x,D) = \sum_{i=1}^N\kappa(x,X_i)Y_i 
%\end{equation*} 
%This is called a Kernal smoother. 

\end{itemize}




\end{document}







%----------------------------------------------------------------------------------------------------------------
%\section{Regularization view of priors}
%\begin{itemize}
%\item There is 
%\end{itemize}


%----------------------------------------------------------------------------------------------------------------
%\section{Kernel based view}
%\begin{itemize}
%\item Suppose we have a 
%\end{itemize}


%----------------------------------------------------------------------------------------------------------------
%\section{Regularization}
%\begin{itemize}
%\item We've seen that adding flexibility to our models comes at the expense of precision, or variance in our predictions. Is there some way to build models that are very flexible, but somehow still constrained so as not to be to be very variable? One approach is known as \dfn{regularization}. This is especially important if we want to add more parameters than data points. 
%\item To understand regularization, recall that our current fitting procedure for a regression model computes $\hat{\beta}_i$ from 
%\begin{equation*}
%{\rm min}_{\beta} \sum_{i=1}^N(\hat{y}(X_i,D) - Y_i)^2
%\end{equation*}
%The idea of regularization is to 
%\end{itemize}





 \bibliographystyle{unsrt}
\bibliography{./../refs.bib}








