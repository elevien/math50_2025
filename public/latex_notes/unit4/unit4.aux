\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Multiple predictor linear regression}{1}{section.1}\protected@file@percent }
\citation{gelman2020regression}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Basic interpretation and estimation of the parameters }{2}{subsection.1.1}\protected@file@percent }
\newlabel{eq:2dsurface}{{6}{2}{Basic interpretation and estimation of the parameters}{equation.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The function $y(x_1,x_2)$}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:plane}{{1}{3}{The function $y(x_1,x_2)$}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Relationship between single and two predictor regression coefficients}{5}{subsection.1.2}\protected@file@percent }
\newlabel{eq:beta1exp}{{8}{5}{Relationship between single and two predictor regression coefficients}{equation.1.8}{}}
\newlabel{eq:beta-single-multiA}{{9}{6}{Relationship between single and two predictor regression coefficients}{equation.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Effect of adding predictors on $R^2$}{6}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Covariance matrix and estimation of regression coefficients}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Regression coefficients in terms of the covariance matrix}{6}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Here I'm illustrated the difference between the marginal regression slope (the slope of $E[Y|X_1]$ vs. $X_1$) and the regression coefficient $\beta _1$ in the two predictor model. Here I use $b$ to denote the regression coefficient of $X_1$ with $X_2$ as a predictor. I use the notation of Example \ref  {ex:normal_pred}, although the idea applies more generally. When we increase $x_1$ by $1$ without fixing $X_2$, then on average $X_2$ changes by $b$ (which is the slope between $x_1$ and $x_2$ here, not the intercept.) Therefore, in order to relate this marginal slope to the regression slope $\beta _1$, subtract the increase in $Y$ that is caused by the increase in $X_2$ (corresponding to the vertical blue arrow). }}{7}{figure.2}\protected@file@percent }
\newlabel{fig:plane}{{2}{7}{Here I'm illustrated the difference between the marginal regression slope (the slope of $E[Y|X_1]$ vs. $X_1$) and the regression coefficient $\beta _1$ in the two predictor model. Here I use $b$ to denote the regression coefficient of $X_1$ with $X_2$ as a predictor. I use the notation of Example \ref {ex:normal_pred}, although the idea applies more generally. When we increase $x_1$ by $1$ without fixing $X_2$, then on average $X_2$ changes by $b$ (which is the slope between $x_1$ and $x_2$ here, not the intercept.) Therefore, in order to relate this marginal slope to the regression slope $\beta _1$, subtract the increase in $Y$ that is caused by the increase in $X_2$ (corresponding to the vertical blue arrow)}{figure.2}{}}
\newlabel{eq:betacov1}{{12}{7}{Regression coefficients in terms of the covariance matrix}{equation.2.12}{}}
\newlabel{eq:beta1cov}{{15}{8}{Regression coefficients in terms of the covariance matrix}{equation.2.15}{}}
\newlabel{ex:normal_pred}{{4}{8}{Correlated predictors}{example.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Simpson's paradox}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Estimating regression coefficients}{10}{subsection.2.3}\protected@file@percent }
\newlabel{eq:design-nk}{{16}{10}{Estimating regression coefficients}{equation.2.16}{}}
\newlabel{eq:emp-cov}{{17}{10}{Estimating regression coefficients}{equation.2.17}{}}
\newlabel{eq:beta-inv}{{18}{10}{Estimating regression coefficients}{equation.2.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Moore--Penrose pseudoinverse.}{10}{section*.2}\protected@file@percent }
\newlabel{eq:beta-pinv-nk}{{19}{10}{Moore--Penrose pseudoinverse}{equation.2.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Sample distribution and collinearity}{10}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}The sample distribution}{10}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (top) In the single-predictor case, the width of the sample distribution measures how confident we are of a particular slope. It will be narrow if a replicate of our data is likely to produce a very similar slope. These means we get a rough idea of the width of sample distribution by seeing much we can change our regression line and still obtain something that appears to pass through our data. (bottom) In the two predictor case, we have a regression plane and changing $\beta _1$ and $\beta _2$ will ``wiggle" the plane by tilting it in the $x_1$ and $x_2$ directions (there is also the intercept which can shift the plane up and down, but I'm not illustrating that). If $X_1$ and $X_2$ are uncorrelated, it doesn't matter which way we wiggle it, the fit will be similar, but if $X_1$ and $X_2$ are strongly correlated, wiggling the plane in the direction perpendicular to the points has a much smaller effect that parallel to them. }}{11}{figure.3}\protected@file@percent }
\newlabel{fig:sample_dist}{{3}{11}{(top) In the single-predictor case, the width of the sample distribution measures how confident we are of a particular slope. It will be narrow if a replicate of our data is likely to produce a very similar slope. These means we get a rough idea of the width of sample distribution by seeing much we can change our regression line and still obtain something that appears to pass through our data. (bottom) In the two predictor case, we have a regression plane and changing $\beta _1$ and $\beta _2$ will ``wiggle" the plane by tilting it in the $x_1$ and $x_2$ directions (there is also the intercept which can shift the plane up and down, but I'm not illustrating that). If $X_1$ and $X_2$ are uncorrelated, it doesn't matter which way we wiggle it, the fit will be similar, but if $X_1$ and $X_2$ are strongly correlated, wiggling the plane in the direction perpendicular to the points has a much smaller effect that parallel to them}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Sample distribution formula}{11}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Dealing with categorical data}{11}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Different views of the data in the case when $X_1$ and $X_2$ are correlated. If we look at the data from the side, or along the $X_1=X_2$ direction, then all our regression planes appear similar; however, when looked at from the ``front" as shown in the right panel, we see that the places actually have very different slopes in the other direction. }}{12}{figure.4}\protected@file@percent }
\newlabel{fig:sample_dist2}{{4}{12}{Different views of the data in the case when $X_1$ and $X_2$ are correlated. If we look at the data from the side, or along the $X_1=X_2$ direction, then all our regression planes appear similar; however, when looked at from the ``front" as shown in the right panel, we see that the places actually have very different slopes in the other direction}{figure.4}{}}
\citation{demidenko2019advanced}
\newlabel{ex:binnorm}{{1}{14}{A binary and normal predictor \ding {111}}{exercise.1}{}}
\newlabel{ex:sampledist}{{3}{15}{Sample distribution \ding {111}}{exercise.3}{}}
\bibstyle{unsrt}
\bibdata{./../refs.bib}
\bibcite{gelman2020regression}{1}
\bibcite{demidenko2019advanced}{2}
\gdef \@abspage@last{16}
