\include{./../latex/notes_style.tex}
%--------------------------------------------------------------------------------------------------------------------------------
\setcounter{unit}{4}
\setcounter{section}{0}
\begin{document}



\title{Unit 4: Regression with multiple predictors}
\author{Ethan Levien}
\maketitle


%----------------------------------------------------------------------------------------------------------------
\section*{Introduction}
Now we are ready to study linear regression with multiple predictors. Much of the concepts carry over from the single predictor case and the Python code is nearly the same. There are some new aspects though when it comes to how we interpret the predictors. In particular, we have to remember that these represent average differences in the response variable when all other predictors are fixed. This is the idea of controlling for another variable. We will also understand what happens when we add regression coefficients to the model. 

\section{Multiple predictor linear regression}
The real power of regression comes when we work with models of the form 
\begin{align}
Y &= \beta_0 + \sum_{i=1}^K \beta_iX_i + \epsilon\\
\epsilon &\sim {\rm Normal}(0,\sigma^2)
\end{align}
where $X_i$ is a set of $K$ predictor variables. Alternatively, we can write
\begin{equation}
Y|(X_1=x_1,\dots,X_K=x_K) \sim {\rm Normal}\left( \beta_0 + \sum_{i=1}^K \beta_iX_i,\sigma^2\right)
\end{equation}
You might see the shorthand, 
\begin{equation}
Y \sim {\rm LR}(X,\beta,\sigma^2). 
\end{equation}

In these notes, our goal is to answer the following questions
\begin{enumerate}
\item What are estimators of the parameters in this model?
\item How do we interpret the \dfn{regression coefficients} $\beta_i$? 
\item What is the sample distribution of the regression coefficients? 
\item How do the correlations between predictors influence the answers to these questions? 
\end{enumerate}



\begin{example}[Two-predictor linear regression in \texttt{statsmodels}]
\noindent\underline{Question:} Generate data from the two-predictor model
\[
Y \;=\; \beta_0 \;+\; \beta_1 X_1 \;+\; \beta_2 X_2 \;+\; \varepsilon,\qquad \varepsilon\sim\mathcal N(0,\sigma^2),
\]
and fit the linear regression \(Y\sim X_1+X_2\) using \texttt{statsmodels}.

\medskip
\noindent\underline{Solution:}
\begin{lstlisting}[language=Python]
import numpy as np
import statsmodels.api as sm

# --- simulate data ---
n = 300
rng = np.random.default_rng(0)

beta0, beta1, beta2, sigma = 1.0, 2.0, -1.0, 0.5
X1 = rng.normal(size=n)
X2 = rng.normal(size=n)
eps = rng.normal(scale=sigma, size=n)
Y = beta0 + beta1*X1 + beta2*X2 + eps

# --- fit OLS: Y ~ X1 + X2 ---
X = sm.add_constant(np.column_stack([X1, X2]))
model = sm.OLS(Y, X)
results = model.fit()
\end{lstlisting}
\end{example}

The output from the regression with multiple predictors is basically the same as for single-predictor, except now we have multiple rows for the difference regression coefficients. In each case, the interpretation of the $p$-value and confidence intervals are nearly the same as they were for the single predictor case. However, for the $p$-value, we need to remember that this is the $p$-value testing the hypothesis that a particular predictor is zero. The $F$-statistic is used to test the hypothesis that all predictors are zero, although I won't go into much more detail because I don't place a big emphasis on hypothesis testing in this course. 

 The interpretation of $R^2$ is the same as before, except that now we are considering the ratio of the variance conditioned on ALL predictors to the overall variation in $Y$; that is, 
\begin{equation*}
R^2 = 1 - \frac{\sum_i r_i^2}{\sum_i (y_i - \bar{y})^2}    \approx 1 - \frac{{\rm var}(Y|X_1,X_2)}{{\rm var}(Y)}
\end{equation*} 
where in the multi-predictor case 
\begin{equation}
r_i = Y_i - \left(\hat{\beta}_0 + \sum_k^m \hat{\beta}_kX_{i,k}\right). 
\end{equation} 



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{./../figures/plane}
    \caption{The function $y(x_1,x_2)$}
    \label{fig:plane}
\end{figure}

\subsection{Basic interpretation and estimation of the parameters }
\sectionrefs{ \cite[Ch. 10]{gelman2020regression}}
 In order to interpret the parameters, it's easiest to work with just two predictors like we have in the example above. The formula for the conditional expectation of $Y$ is 
\begin{equation}\label{eq:2dsurface}
E[Y|X] = \beta_0 + \beta_1X_1 + \beta_2X_2
\end{equation}
where I'm using the shorthand 
\begin{equation*}
E[Y|X] = E[Y|(X_1,X_2)]
\end{equation*}
to mean the expected value of $Y$ conditioned on {\bf both} predictors. 



Equation \ref{eq:2dsurface} is the equation for a flat surface in two dimensions: 
\begin{equation}
y = \beta_0 + \beta_1x_1 + \beta_2x_2
\end{equation}
A drawing of $y$ is shown in \ref{fig:plane}. 

 If we make a slice through the surface in the $x_1$ direction and look it at from the side, we see a line with slope $\beta_1$ (and similarly for $x_2$).  This leads to the following interpretation of $\beta_i$:  
 \begin{center}
 $\beta_1$ is the slope of $E[Y|X]$ vs. $X_1$ for fixed $X_2$. 
 \end{center}
Notice that in the statement above, even though we are conditioning on both variables, the slope $\beta_1$ is independent of which value of $X_2$ we condition on. We can obtain the interpretation of $\beta_2$ by flipping the role of $X_1$ and $X_2$. 
The fact that is doesn't matter which value of $X_2$ (respectively $X_1$) we have conditioned on is a consequence of linearity, and thus one of the core model assumption of linear regression with multiple predictors, which we do not encounter in the single predictor case. Another way of articulating it is to say: the ``effect'' of $X_1$ and $X_2$ are not dependent on the other predictor's value.





\begin{example}[Test scores]
We will now work with a new example of Children's test scores. To motivate this, we can imagine we are interested in studying what factors determine children's success in school in order to effective design interventions which help students that are struggling. The predictors are mother IQ and high school education.  
 In this case, the model assumptions are saying that the association between the mother's high school education and test scores is not influenced by the mother's IQ. that is, If we compare two random children whose mothers have the same IQ, differ in whether they attended high school, then the average \emph{difference} between their test scores will not depend on the IQ of their mothers, although the average magnitude of their test scores will depend on the mother's IQ. 


\noindent
\underline{Question:} Fit the data to a linear regression model with two predictors and answer the questions
\begin{enumerate}[label=(\alph*)]
\item What are the regression coefficients and the interpretations? 
\item Based on this regression analysis, which factor, IQ or high school education do we believe is more predictive of test scores? 
\item Overall, how well do high school education and IQ as methods do at predicting the test scores of children? 
\item What is the chance a student whose mother has an IQ of $90$ and did not go to high school does better than a student whose mother has an IQ of $110$ and did go to high school?\\
\end{enumerate}



\noindent
\underline{Solutions:}
We get the following output from \verb!statsmodels! in the \href{https://colab.research.google.com/drive/1oIRgP_7-c5DGV1D2iz5nj406mZfJxUIG?usp=sharing}{colab notebook}:\\

\noindent
\begin{Verbatim}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.214                                   
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         25.7315      5.875      4.380      0.000      14.184      37.279
mom_hs         5.9501      2.212      2.690      0.007       1.603      10.297
mom_iq         0.5639      0.061      9.309      0.000       0.445       0.683


\end{Verbatim}
\vspace{1cm}
\begin{enumerate}[label=(\alph*)]
\item For the regression coefficients we find the follow:
\begin{itemize}
\item $\beta_{\rm hs} \approx 5.95$. This means that among students whose mothers {\bf have the same IQ}, a student whose mother attended high school will, on average, have a score that is $5.95$ points higher than a student whose mother did not. 
\item $\beta_{\rm iq} \approx 0.56$. This means that among students whose mother's {\bf have the same high school education} (either they all attended or did not attend high school), the difference between scores of students whose mothers IQ differs by one point is, on average, $0.56$ points.
\item $\hat{\beta}_0 \approx 26$. Mathematically, this tells us the average score of students whose mother did not attend high school and have zero IQ, but this is not a meaningful quantity since noone has zero iq. We can therefore ignore it when it comes to interpreting the output. 
\end{itemize}
\item Clearly $\beta_{\rm hs}$ is smaller, but we need to remember that are comparing quantities that have different units. $X_{\rm iq}$ takes values from around 70 to 130, while $X_{\rm hq}$ is either zero or 1. What is actually more useful is to compare how much a difference in one standard deviation of the predictor makes. For example, $\beta_{\rm iq}\sigma_{\rm iq}$ is the average difference in test scores between students whose mothers have the same high school education, but whose mother's IQ differ by one standard deviation. To this end, we can compute the following measures of effects
\begin{align*}
\hat{\beta}_{\rm hs}\hat{\sigma}_{\rm hs} &\approx 2.44\\
\hat{\beta}_{\rm iq}\hat{\sigma}_{\rm iq} &\approx 8.44.
\end{align*}
The association between IQ and scores is actually larger. Note that the comparison is not perfect, since $X_{\rm hs}$ is a binary variable, but it still gives us a generally idea of the effects. \\
\item The $R^2$ value is $0.214$, so about $20\%$ of the variation in test scores is explained by the variation in high school education and IQ of mothers. 
\item In the colab notebook we calculate this to be about $25\%$. 
\end{enumerate}
\end{example}

The interpretation can of course be generalized to the situation where we have many predictors. The general formula for the regression coefficient in terms of expectation is 
\begin{align*}
\begin{split}
\beta_i &= E[Y|X_1,\dots,X_{i-1},X_i=x_i+1,X_{i+1},\dots,X_K]\\
&\quad\quad - E[Y|X_1,\dots,X_{i-1},X_i=x_i,X_{i+1},\dots,X_K]
\end{split}
\end{align*}
Note how this is a very natural extension of Equation \ref{eq:beta1exp}. 
We get a more complex expression for the coefficients but the idea is the same. 


\subsection{Relationship between single and two predictor regression coefficients}

 We can express the regression coefficients explicitly in terms of conditional averages as
 \begin{equation}\label{eq:beta1exp}
 \beta_1 = E[Y|X_1 = (x+1),X_2] -  E[Y|X_1 = x,X_2].
 \end{equation}
Now let's think about how the regression coefficients are related to covariance. One guess would be that, just as in the single-predictor case, $\beta_1$ is given by ${\rm cov}(Y,X_1)/\sigma_{x_1}^2$. After all, if we look a slice of the 2D planer function $y(x_1,x_2)$ along the $x_1$ direction, we get the same slope for all $x_2$.  It stands to reason that if we look at only the points in the $x_1$-$y$ plane our regression slope would be $\beta_1$. However, {\bf this argument assumes that when we change $x_1$, $x_2$ does not also change}. This is best understood with an example. 


\begin{example}[Test scores with multiple vs. single predictors]
Here we will consider once again the example of children's test scores and compare using both predictors in the sample above to the results we obtain we using only one predictor (high school education). \\



\noindent
\underline{Question:} What is the difference between the coefficient of $X_{\rm hs}$ when this is the only predictor and the coefficient when $X_{\rm iq}$ is also used? How is the coefficient in the multiple predictor case related to coefficient in the single predictor case?  \\

\noindent
\underline{Solution:} When we performed the regression using only the mother's high school education as a predictor, we obtained a coefficients of about $\hat{\beta}_{\rm hs}' \approx 12$ and $\hat{\beta}_0' \approx 78$ (i'll use $\beta'$ indicate coefficients in the single predictor model, as opposed to the multiple predictor model). The fitted model is
\begin{equation*}
\hat{y} = 12X_{\rm hs} + 78 
\end{equation*}
while when also using $X_{\rm iq}$ as a predictor,  the coefficient is about half that.


In the model with one predictor, the regression coefficient of $12$ means that on average a student whose mother went to high school will do $12$ points better than one whose mother did not.  That is, we are predicting
\begin{equation*}
E[Y|X_{\rm hs} = 1]-E[Y|X_{\rm hs} = 0]  = \beta_{\rm hs}' \approx 12
\end{equation*}
Let's compare this to what we would predict in the model with two predictors. 
In that case, the average test score of student whose mother went to high school is
\begin{align*}
\hat{y}_{\rm hs} &\approx E[Y|X_{\rm hs} =1]\\
&=  E[\beta_0 +  \beta_{\rm hs} +\beta_{\rm iq}X_{\rm iq} |X_{\rm hs}=1]\\
&=\beta_0 +  \beta_{\rm hs} +\beta_{\rm iq}E[X_{\rm iq}|X_{\rm hs}=1]\\
&\approx  6\times 1+ 26  +  0.6 \overline{X}_{{\rm iq}|{\rm hs}} 
\end{align*}
where 
\begin{equation*}
\overline{X}_{{\rm iq}|{\rm hs}}  =  \text{sample average IQ of mother who attended high school} \approx E[X_{\rm iq}|X_{\rm hs}=1]
\end{equation*}

On the other hand 
\begin{equation*}
\hat{y}_{\rm no-hs} =  6 \times 0 + 26  +  0.6 \overline{X}_{{\rm iq},{\rm no-hs}} 
\end{equation*}
where 
\begin{equation*}
\overline{X}_{{\rm iq}|{\rm no-hs}}  = \text{sample average IQ of mother who DID NOT attend high school} \approx  E[X_{\rm iq}|X_{\rm hs}=0]
\end{equation*}


Thus, according to the model with two predictors, the average difference in test scores between the \verb!hs! and \verb!no-hs! groups is 
\begin{equation*}
\Delta \hat{y}_{\rm hs} = 6 +0.6(\overline{X}_{{\rm iq}|{\rm hs}}-\overline{X}_{{\rm iq}|{\rm no-hs}} )
\end{equation*}
or written in terms of more probabilistic notation 
\begin{equation*}
E[Y|X_{\rm hs} = 1]-E[Y|X_{\rm hs} = 0]  = \beta_{\rm hs} + \beta_{\rm iq}(E[X_{\rm iq}|X_{\rm hs} = 1]-E[X_{\rm iq}|X_{\rm hs} = 0]  )
\end{equation*}
We can compute $\overline{X}_{{\rm iq}|{\rm hs}}-\overline{X}_{{\rm iq}|{\rm no-hs}} \approx 10.3$, which gives $\Delta \hat{y}_{\rm hs} \approx 12$. Thus, we have calculated the single-predictor regression coefficient from the multiple predictor case.  





\end{example}




  
%  \begin{example}
%\href{https://colab.research.google.com/drive/1oIRgP_7-c5DGV1D2iz5nj406mZfJxUIG#scrollTo=wbeO1TS8os5J&line=15&uniqifier=1}{Understanding the multiple predictors regression slopes}
%\end{example}
  
The important thing is that the two predictors are not independent. If they were, then $\overline{X}_{{\rm iq}|{\rm hs}}-\overline{X}_{{\rm iq}|{\rm no-hs}}$ would be zero, and it would have to be that the coefficient of $X_{\rm hs}$ is the same in both cases. We can generalize this to any model where $X_1$ is a binary predictor to obtain a relationship between the regression coefficient for $\beta_1$ with and without the second predictor; that is, 
\begin{equation}\label{eq:beta-single-multiA}
\beta_1' = \beta_1 + \beta_2(E[X_2|X_1=1]-E[X_2|X_1=0])
\end{equation}
where $\beta_1'$ is the regression coefficient without using $X_2$ as a predictor in our model. 
 
 \subsection{Effect of adding predictors on $R^2$}

Adding a new predictor to a regression model can never decrease the coefficient of determination $R^2$.  
Let $R_{1\,{\rm pred}}^2$ denote the $R^2$ from the single-predictor model
\[
Y = \beta_1 X_1 + \epsilon',
\]
and $R_{2\,{\rm pred}}^2$ the $R^2$ from the two-predictor model
\[
Y = \beta_1 X_1 + \beta_2 X_2 + \epsilon.
\]
By definition, $R^2 = 1 - {\rm Var}(\text{residual})/{\rm Var}(Y)$, so $R^2$ increases whenever the residual variance decreases.

The key point is that the new error term $\epsilon'$ in the reduced model is \emph{not} the same as $\beta_2 X_2 + \epsilon$.  
Instead, $\epsilon'$ absorbs the variation in $Y$ that is correlated with $X_2$ but unaccounted for by $X_1$.  
Because $X_1$ and $X_2$ are generally correlated, part of the systematic variation explained by $X_2$ is treated as noise in the single-predictor model. Rather we have
\begin{equation}
{\rm var}(\epsilon') = {\rm var}(Y|X_1) = {\rm var}(\beta_1 X_1 + \beta_2 X_2 + \epsilon|X_1) = \beta_1^2{\rm var}(X_2|X_1) + \sigma_{\epsilon}^2 > \sigma_{\epsilon^2}
\end{equation}
hence
\[
R_{2\,{\rm pred}}^2 \;\ge\; R_{1\,{\rm pred}}^2,
\]
with equality only if $X_2$ is orthogonal to both $X_1$ and $Y$ (i.e.\ it provides no additional explanatory power).  
Thus, adding predictors either improves the fit or leaves it unchanged, but never worsens it.

 \section{Covariance matrix and estimation of regression coefficients}
 
 \subsection{Regression coefficients in terms of the covariance matrix}
Here we will derive formulas for the regression coefficients in terms of covariances between the predictors, and covariance between the predictors and the response variable. This will (1) allow us to better understand the relationship between single and multiple predictor regression coefficients and (2) lead to an estimator the regression coefficients in the multiple predictor case. In particular, we obtain formulas that generalize the relationship ${\rm cov}(X,Y) = \beta_1\sigma_x^2$, which we discovered to hold in the single predictors case. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./../figures/correlated_predictors}
    \caption{Here I'm illustrated the difference between the marginal regression slope (the slope of $E[Y|X_1]$ vs. $X_1$) and the regression coefficient $\beta_1$ in the two predictor model. Here I use $b$ to denote the regression coefficient of $X_1$ with $X_2$ as a predictor. I use the notation of Example \ref{ex:normal_pred}, although the idea applies more generally. When we increase $x_1$ by $1$ without fixing $X_2$, then on average $X_2$ changes by $b$ (which is the slope between $x_1$ and $x_2$ here, not the intercept.) Therefore, in order to relate this marginal slope to the regression slope $\beta_1$, subtract the increase in $Y$ that is caused by the increase in $X_2$ (corresponding to the vertical blue arrow).   }
    \label{fig:plane}
\end{figure}

 Consider a linear regression model with two predictors. The formulas we derive will generalize to many predictors. We will set $\beta_0=E[X_1]=E[X_2]=0$ for simplicity, since these cancels out in the end. An practice, I recommend checking that everything works out when this are not zero! We start by computing ${\rm cov}(X_1,Y)$, which is simply $E[X_1Y]$ since $E[X_1]=E[Y] = 0$. 
%\begin{equation*}
%E[Y] = \beta_0 + \beta_1E[X_1]+\beta_2E[X_2]=0. 
%\end{equation*}
Just as we did for the single-predictor case in Unit 2, we write
\begin{align*}
{\rm cov}(X_1,Y) &= E[X_1Y] = E[X_1E[Y|X_1]] \\
&= E[X_1(\beta_1 X_1 + \beta_2 X_2)] = \beta_1 E[X_1^2] + \beta_2 E[X_1X_2]\\
&= \beta_1 \sigma_{x_1}^2  + \beta_2 {\rm cov}(X_1,X_2)
\end{align*}
where we have used that, since $E[X_1]=E[X_2]=0$, ${\rm var}(X_1) = E[X_1^2]$ and ${\rm cov}(X_1,X_2) = E[X_1X_2]$.
 If we do the same for $X_2$, we get two equations
 \begin{align*}
 {\rm cov}(X_1,Y)  &= \beta_1 \sigma_{x_1}^2  + \beta_2 {\rm cov}(X_1,X_2)\\
{\rm cov}(X_2,Y) &=   \beta_2 \sigma_{x_2}^2  + \beta_1 {\rm cov}(X_1,X_2)
 \end{align*}
Notice that as with the single-predictor case, it is useful to represent $\beta_1$ and $\beta_2$ as expectations which can be computed as averages over our data points. In addition to providing some insight into the meaning of the regression coefficients, this will yield candidates for our estimators of these quantities. This formulas can be rewritten in terms of a matrix, known as the \dfn{covariance matrix}
\begin{equation}
\Sigma =  \left[\begin{array}{cc}
\sigma_{x_1}^2 & {\rm cov}(X_1,X_2)\\
 {\rm cov}(X_1,X_2) & \sigma_{x_2}^2
 \end{array} \right].
\end{equation}
We then have 
\begin{align}\label{eq:betacov1}
\left[\begin{array}{c}
{\rm cov}(X_1,Y) \\
{\rm cov}(X_2,Y)  
\end{array}\right] &= \Sigma \left[ \begin{array}{c}
\beta_1\\
  \beta_2\\
  \end{array}\right]
\end{align}
This is consistent with Eq. \ref{eq:beta-single-multiA}. In particular, by taking the first equation in this system and dividing by ${\rm var}(X_1)$, we can rewrite Eq. \ref{eq:beta-single-multiA} as
\begin{equation}
\beta_1' = \beta_1  + \beta_2 \beta_{1,2}
\end{equation}
where $\beta_{1,2} = {\rm cov}(X_1,X_2)/{\rm var}(X_1)$ is the (single-predictor) regression coefficient of $X_1$ with $X_2$ as the response variable. Eq. \ref{eq:betacov1} can easily be generalized to many predictors. In the general case, the covariance matrix $\Sigma$ is a $K \times K$ matrix with entries $\Sigma_{i,j} = {\rm cov}(X_i,X_j)$. 

Letting $\Sigma^{-1}$ be the inverse of $\Sigma$, meaning 
\begin{equation}
\Sigma^{-1}\Sigma = I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\end{equation}
we then have 
\begin{align*}
\left[\begin{array}{c}
\beta_1 \\
\beta_2 
\end{array}\right] &= \Sigma^{-1}\left[ \begin{array}{c}
 {\rm cov}(X_1,Y)\\
  {\rm cov}(X_2,Y)\\
  \end{array}\right]\\
  &=  \frac{1}{\sigma_{x_2}^2\sigma_{x_1}^2 -{\rm cov}(X_1,X_2)^2} \left[\begin{array}{cc}
\sigma_{x_2}^2 & -{\rm cov}(X_1,X_2)\\
- {\rm cov}(X_1,X_2) & \sigma_{x_1}^2
 \end{array} \right]\left[ \begin{array}{c}
 {\rm cov}(X_1,Y)\\
  {\rm cov}(X_2,Y)\\
  \end{array}\right]
\end{align*}
 You should notice the similarity between this formula and the single predictor formula $\beta_1' = {\rm cov}(X,Y)/{\rm var}(X)$. Here, $\Sigma^{-1}$ is playing the role of $1/{\rm var}(X)$, while the vector of $\beta$s and $Y$-$X$ covariances play the roles of $\beta_1$ and ${\rm cov}(X,Y)$ respectively. Notice that if $\Sigma = I$, then we can solve for each $\beta_i$ separately and we obtain the same formulas as the single predictor case. 
 
 
We don't usually worry about the closed form solutions for the $\beta$ because they become impossibly complicated as the number of predictors grows. However, for two predictors we can derive a formula for $\beta_1$, which is 
\begin{align}\label{eq:beta1cov}
\beta_1 =     \frac{ {\rm cov}(X_1,Y)\sigma_{x_2}^2 - {\rm cov}(X_2,Y){\rm cov}(X_1,X_2)}{\sigma_{x_2}^2\sigma_{x_1}^2 -{\rm cov}(X_1,X_2)^2}
%&= \frac{ {\rm cov}(X_1,Y)- {\rm cov}(X_2,Y){\rm cov}(X_1,X_2)/\sigma_{x_2}^2 }{\sigma_{x_1}^2 -{\rm cov}(X_1,X_2)^2/\sigma_{x_2}^2 }
\end{align}
%We can rewrite this as
%\begin{align*}
%\beta_1 =    \frac{ {\rm cov}(X_1,Y)\sigma_{x_2}^2 - {\rm cov}(X_2,Y){\rm cov}(X_1,X_2)}{\sigma_{x_2}^2\sigma_{x_1}^2 -{\rm cov}(X_1,X_2)^2}
%\end{align*}\
The formula is particularly revealing if all the variances are set to one
\begin{equation*}
\beta_1 = \frac{1}{1-\rho_{1,2}^2}(\rho_1 - \rho_{1,2}\rho_2)
\end{equation*}
where $\rho_{1,2}$ is the correlation coefficient between $X_1$ and $X_2$. Notice that if $X_1$ and $X_2$ are uncorrelated ($\rho_{1,2}=0$), we obtain the usual connection between the regression coefficient and the correlation coefficient between $X_1$ and $X_2$. 

\begin{example}[Correlated predictors]\label{ex:normal_pred}
Consider the model
\begin{align*}
X_1 &\sim {\rm Normal}(0,1)\\
X_2|X_1 &\sim {\rm Normal}(bX_1,1-b^2)\\
Y|(X_1,X_2) & \sim {\rm Normal}(\beta_1X_1 + \beta_2X_2,\sigma^2). 
\end{align*}
where $b \in [0,1]$. Note that $b$ is just $\beta_{1,2}$ -- the regression coefficient of $X_1$ on $X_2$, which in this case is the same as the correlation coefficient (see below). 


\noindent
\underline{Question:} 
\begin{enumerate}[label=(\alph*)]
\item Show that ${\rm var}(X_1) = {\rm var}(X_2) = 1$ and ${\rm cov}(X_1,X_2)=b$
\item Expression $\beta_1$ as a function of $b$. 
\item Obtain the single predictor regression coefficient $\beta_1'$ when only $X_1$ is the predictor. 
\end{enumerate}



\noindent
\underline{Solution:} 
\begin{enumerate}[label=(\alph*)]
\item By definition of the model ${\rm var}(X_1)=1$ and 
\begin{align*}
{\rm var}(X_2) &= b^2{\rm var}(X_1) + 1-b^2 = b^2+1-b^2 = 1\\
{\rm cov}(X_1,X_2) &= b{\rm var}(X_1) = b
\end{align*}
\item We can write Equation \ref{eq:beta1cov} as
\begin{equation*}
\beta_1 =  \frac{ {\rm cov}(X_1,Y)- {\rm cov}(X_2,Y)b}{1 -b^2}
\end{equation*}
\item The single-predictor slope of $Y$ on $X_1$ is $\beta_1 + b \beta_2$. 
\end{enumerate}

%\noindent
%\underline{Solution:} In order to apply Equation \ref{eq:beta1cov}, we need to determine $\sigma_{x_1}$, $\sigma_{x_2}$ and the covariances. In the notation of the problem $\sigma_{x_1} = \sigma_1$ and setting $\sigma_2 = \sigma_{x_2}$, we have
%\begin{align*}
%\sigma_{2}^2 &=  b^2\sigma_{1}^2 + \sigma_{2|1}^2\\
%{\rm cov}(X_1,X_2) &= b\sigma_1\\
%{\rm cov}(X_1,Y) &= 
%\end{align*}

\end{example}


\subsection{Simpson's paradox}
In the example above we can see that $\beta_1'$ and $\beta_1$ can have different signs depending on $b$. This effect is called \dfn{Simpson's ``paradox''}. This is not really a paradox, but a simple consequence of the fact that the correlation between predictors influences the single-predictor regression coefficient as illustrated in Figure \ref{fig:plane}. Let's look at a another example. 

\begin{example}
Consider two binary predictors \(X_1, X_2 \in \{0,1\}\) with joint distribution given by the table of probabilities:

\[
\begin{array}{c|cc}
P(X_1, X_2) & X_2=0 & X_2=1 \\ \hline
X_1=0 & 0.4 & 0.1 \\
X_1=1 & 0.1 & 0.4
\end{array}
\]

Suppose the outcome \(Y\) satisfies the linear model

\[
Y \mid (X_1, X_2) = X_1 -2 X_2 + \epsilon 
\]


\noindent
\underline{Question:} Compute the single-predictor regression coefficient $\beta_1'$ of $X_1$ on $Y$. 


\noindent
\underline{Solution:}
First compute the difference in conditional means of \(X_2\) across \(X_1\):
\begin{align*}
\mathbb P(X_2=1\mid X_1=1) &= \frac{0.4}{0.1+0.4}=0.8,\qquad
\mathbb P(X_2=1\mid X_1=0) = \frac{0.1}{0.4+0.1}=0.2, \\
\beta_{1,2} &\equiv \mathbb E[X_2\mid X_1=1]-\mathbb E[X_2\mid X_1=0] = 0.8-0.2=0.6
\end{align*}
With the true model \(Y=X_1+(-2)X_2+\varepsilon\) (so \(\beta_1=1,\ \beta_2=-2\)), the single-predictor slope satisfies
\begin{equation*}
\beta_1' \;=\; \beta_1 + \beta_2\beta_{1,2}. 
\end{equation*}
Substituting \(\Delta_{2\mid 1}=0.6\) yields
\begin{equation*}
\beta_1' \;=\; 1 + (-2)\cdot 0.6 \;=\; 1-1.2 \;=\; -0.2. 
\end{equation*}

\end{example}




\subsection{Estimating regression coefficients}
Now suppose we have data points $\{(X_{i,1},\dots,X_{i,K},Y_i)\}_{i=1}^N$. 
We form the \dfn{design matrix} with \emph{samples in rows} and \emph{predictors in columns}:
\begin{equation}
X \;=\;
\begin{bmatrix}
X_{1,1} & \cdots & X_{1,K}\\
\vdots  & \ddots & \vdots \\
X_{N,1} & \cdots & X_{N,K}
\end{bmatrix}
\in\mathbb{R}^{N\times K},
\qquad
Y \;=\; \begin{bmatrix} Y_1 \\ \vdots \\ Y_N \end{bmatrix}\in\mathbb{R}^{N}.
\label{eq:design-nk}
\end{equation}
Assume $X$ and $Y$ are centered (no intercept). The empirical covariance quantities are
\begin{equation}
\hat{\Sigma} \;=\; \frac{1}{N}X^\top X \in \mathbb{R}^{K\times K},
\qquad
\hat{c} \;=\; \frac{1}{N}X^\top Y \in \mathbb{R}^{K}.
\label{eq:emp-cov}
\end{equation}
When $\hat{\Sigma}$ is invertible,
\begin{equation}
\hat{\beta} \;=\; \hat{\Sigma}^{-1}\hat{c}
\;=\; (X^\top X)^{-1}X^\top Y.
\label{eq:beta-inv}
\end{equation}

\paragraph{Moore--Penrose pseudoinverse.}
If $\hat{\Sigma}$ is singular (e.g.\ collinearity or $K>N$), use the Moore--Penrose pseudoinverse:
\begin{equation}
\hat{\beta} \;=\; X^{+}Y,
\qquad
X^{+} \;=\; V\Sigma^{+}U^\top\quad\text{for }X=U\Sigma V^\top\text{ (SVD)},
\label{eq:beta-pinv-nk}
\end{equation}
where $\Sigma^{+}$ inverts nonzero singular values and leaves zeros unchanged.


\section{Sample distribution and collinearity}

Just as before, we want to understand what the sample distribution of the coefficients looks like. In the multiple predictor case, we need to think about the joint distribution  of $(\hat{\beta}_1,\hat{\beta}_2,\dots,\hat{\beta}_K)$. We will start by focusing on the two predictor case.  



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./../figures/sample_dist}
    \caption{(top) In the single-predictor case, the width of the sample distribution measures how confident we are of a particular slope. It will be narrow if a replicate of our data is likely to produce a very similar slope.  These means we get a rough idea of the width of sample distribution by seeing much we can change our regression line and still obtain something that appears to pass through our data. (bottom) In the two predictor case, we have a regression plane and changing $\beta_1$ and $\beta_2$ will ``wiggle" the plane by tilting it in the $x_1$ and $x_2$ directions (there is also the intercept which can shift the plane up and down, but I'm not illustrating that). If $X_1$ and $X_2$ are uncorrelated, it doesn't matter which way we wiggle it, the fit will be similar, but if $X_1$ and $X_2$ are strongly correlated, wiggling the plane in the direction perpendicular  to the points has a much smaller effect that parallel to them. }
    \label{fig:sample_dist}
\end{figure}



\begin{example}[Predictor sample distribution]

Consider the model in \ref{ex:normal_pred}. Let's look at the sample distribution by fitting many simulated replicates. \\

\noindent
\underline{Question:} Write a function to generate a dataframe containing samples from the sample distribution of $(\hat{\beta}_1,\hat{\beta}_2)$. Make a scatter plot and explore the structure of the sample distribution, in particular it dependence on $b$, which controls the correlations between $X_1$ and $X_2$. \\


\noindent
\underline{Solution:} 
See \href{https://colab.research.google.com/drive/1oIRgP_7-c5DGV1D2iz5nj406mZfJxUIG?usp=sharing}{colab notebook}

\end{example}

\subsection{The sample distribution}

To understand more intuitively what is going on, imagine $X_1$ and $X_2$ are very highly correlated (if they are perfectly correlated we say they are \dfn {colinear}). We can then write 
\begin{align*}
Y &= \beta_1X_1 + \beta_2X_2 + \epsilon  \approx \beta_1X_1 + \beta_2X_1 + \epsilon\\
 &\approx (\beta_1+\beta_2)X_1 + \epsilon
\end{align*}
There are many ways to select $\beta_1$ and $\beta_2$ so that the surface $\beta_1x_1+a_2\beta_2$ is close to the lines, since a change in $\beta_1$ can be compensated by a change in $\beta_2$. This means that {\bf if we estimate $\beta_1$ and $\beta_2$ and then generated new data, it would be possible to get a VERY different value of $\hat{\beta}_1$ and $\hat{\beta}_2$, so long as $\hat{\beta}_1 + \hat{\beta}_2$ is close to what we got before}. This is illustrated in Figure \ref{fig:sample_dist} and Figure \ref{fig:sample_dist2}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{./../figures/sample_dist2}
    \caption{Different views of the data in the case when $X_1$ and $X_2$ are correlated. If we look at the data from the side, or along the $X_1=X_2$ direction, then all our regression planes appear similar; however, when looked at from the ``front" as shown in the right panel, we see that the places actually have very different slopes in the other direction.  }
    \label{fig:sample_dist2}
\end{figure}

\subsection{Sample distribution formula}

It can be shown that (assuming the covariance matrix $\Sigma$ is know), 
\begin{equation}
\hat{\beta} \sim {\rm MvNormal}(\sigma_{\epsilon}^2 \Sigma^{-1}/N)
\end{equation}
In particular, if $\Sigma =I$ we obtain the sample distribution for the single-predictor case. 




\section{Dealing with categorical data}
 One situation in which models with multiple predictors frequently arrises is when trying to predict a $Y$ variable based on categorical predictors, such as race. In this case, we need to transform the categories  into numerical values. For example, if there are two categories (e.g. YES and NO) we map our variable to $0$ or $1$. If we have $3$ categories (e.g. White, Black, Other), we might first think to map them to $0$, $1$ and $2$. This has a problem though: A chancge from $1$ to $2$ should not necessarily  correspond to a change from $0$ to $1$. In other words, {\bf there is no clear ordering of the $x$ values}.  Sometimes we refer to such predictors and \dfn{qualitative} rather than \dfn{quantitive}, since they express a quality of our data points instead of a numerical quantity. 
 To address this issue, we create \dfn{dummy variables}. 
In particular, In order to take a categorical variable and transform it into a set of indicator variables in python, we use the python function \verb!get_dummies!. The usage of this is illustrated in the following example. 



\begin{example}[Racial disparities in earnings]
Here we will fit the earnings data to a model with race as a predictor. In particular, we want to know: What is the association between race and earnings among adults in the US?  We will start with a model using only race as a predictor. One way to approach this would be to simply use a binary predictor and consider only 2 race categories (e.g. White and non-White). This is limiting though. Instead, we can create a variable for each rate category we are interested in. In the dataset there are 4 race categories (not sure why these 4, but that's what we'll work with)
\begin{equation*}
\{{\rm Black},{\rm White},{\rm Hispanic},{\rm Other}\}
\end{equation*}
In principle, we could create a binary variable for each one (these are what we call dummy variable), to obtain a model like 
\begin{equation*}
Y = \beta_0 + \beta_{\rm black}X_{\rm black} + \beta_{\rm hispanic}X_{\rm hispanic}+ \beta_{\rm other }X_{\rm other}+ \beta_{\rm white}X_{\rm white} + \epsilon 
\end{equation*}
This is problematic though, since at least one of the predictors above MUST be $1$. This means that the first 3 of the predictors are perfectly correlated with the other one. By the default, python will drop the first predictor (in alphabetical order), leaving us with the model
\begin{equation*}
Y = \beta_0 + \beta_{\rm hispanic}X_{\rm hispanic}+ \beta_{\rm other }X_{\rm other}+ \beta_{\rm white}X_{\rm white} + \epsilon. \\
\end{equation*}

\noindent
\underline{Question:} Fit the data to the model above. What is the expected disparity in earnings between someone who is white and someone who is hispanic. \\

\noindent
\underline{Solution:} 
See \href{https://colab.research.google.com/drive/1UTUUfWTpmazSa40sqLUWAojRi2kGJM-O?usp=sharing}{colab notebook}. To answer the question posed above, we begin with the interpretations of the regression coefficients. In terms of conditional expectation, these are
\begin{align*}
\beta_{\rm white} &= E[Y|X_{\rm white}=1,X_{\rm hispanic}=X_{\rm other}=0]- E[Y|X_{\rm white}=0,X_{\rm hispanic}=X_{\rm other}=0]\\
&= E[Y|\text{someone is white}]- E[Y|\text{someone is black}] \approx 4.9\\
\beta_{\rm hispanic} &= E[Y|X_{\rm hispanic}=1,X_{\rm white}=X_{\rm other}=0]- E[Y|X_{\rm hispanic}=0,X_{\rm white}=X_{\rm other}=0]\\
&= E[Y|\text{someone is hispanic}]- E[Y|\text{someone is black}]\approx -0.7\\
\end{align*}
Our goal however is to compute 
\begin{align*}
&E[Y|\text{someone is white}]-E[Y|\text{someone is hispanic}] \\
&= E[Y|X_{\rm white}=1,X_{\rm hispanic}=0,X_{\rm other}=0 ]
-E[Y|X_{\rm white}=0,X_{\rm hispanic}=1,X_{\rm other}=0 ]\\
&= \beta_0 + \beta_{\rm white} - \beta_0 - \beta_{\rm hispanic}\\
& = \beta_{\rm white} - \beta_{\rm hispanic}
\end{align*}



\end{example}




\newpage 

\subsection*{Exercises}


%---------------------------------------------------------------------------------
\begin{exercise}[A binary and normal predictor  \ding{111}]\label{ex:binnorm}
Consider the a linear regression model 
\begin{equation*}
Y|(X_1,X_2) \sim {\rm Normal}(\beta_0 + \beta_1X_1 + \beta_2X_2,\sigma^2)
\end{equation*}
 where the two predictors obey
\begin{align*}
X_1 &\sim {\rm Bernoulli}(q)\\
X_2|X_1 &\sim {\rm Normal}(bX_1,s^2)
\end{align*}
\begin{enumerate}[label=(\alph*)]
\item Can you think are at least two examples where this would be a reasonable model of the relationship between $3$ variables $X_1,X_2$ and $Y$? 
\item What are the formulas for ${\rm cov}(X_1,X_2)$ and ${\rm var}(X_2)$ in terms of the model parameters $q,b,s,\beta_0,\beta_1,\beta_2,\sigma^2$? You should be able to derive these formulas, but you may also reference formulas in previous exercises and class notes. You may assume $\beta_0 = 0$ for this part and the reminder of the exercise, as this simplifies some calculations and doesn't change the results. 
%Then use this to derive a formula for the correlation coefficient between $X_1$ and $X_2$, which we will denote as $\rho_{1,2}$. 
%\item Let 
%\begin{equation*}
%r^2 = 1- \frac{{\rm var}(Y|X)}{{\rm var}(Y)}.
%\end{equation*}
%Recall that this is the quantity which is being approximated by the $R^2$ reported in \verb!statsmodels!. 
%Derive a formula for $r^2$ in terms of the model parameters. 
\item Derive a formula for ${\rm cov}(Y,X_1)$ in terms of $\beta_1$, $q$, $\beta_2$ and $b$.  
\item Explain how the formula you derived in part (b) is related to the equation for ${\rm cov}(Y,X_1)$ in the single predictor regression model (page 4 on week 3 notes). In particular, for what parameter values do the two formulas coincide? Your conclusion will be a particular case of what we saw to be true more generally in class concerning the relationship between $\beta_1$ and the covariances in a regression model with two predictions. 
\item Now derive the formula
\begin{equation*}
{\rm var}(Y) = q(1-q)\left(\beta_1^2 + \beta_2^2b^2 + 2 b \right) + \beta_2^2s^2
+ \sigma^2
\end{equation*}
You will need to use the formula for the variance of the sum of two (not-necessarily independent) random variables, which is given on the midterm practice problems.  
This is also in the ``addition and multiplication section'' on the \href{https://en.wikipedia.org/wiki/Variance#Properties}{wikipedia  page}.
\item The calculations in part (c) allows us to solve an exercise in Chapter 8 in Demidenko's textbook \cite{demidenko2019advanced}, albeit in the more restrictive  context of a binary and normal predictor: 
Is it possible that $\beta_1$ and $\beta_2$ are {\bf both negative}, yet the (marginal) slope of $Y$ vs. $X_1$ is {\bf positive}? If so, generate simulated data where this is the case. 
\end{enumerate}
\end{exercise}





\begin{exercise}[Earnings data] Consider the earnings data. This can be loaded with 
\begin{Verbatim}
df = pd.read_csv("https://raw.githubusercontent.com/avehtari
/ROS-Examples/master/Earnings/data/earnings.csv")
\end{Verbatim}
As in the previous exercise set, you will study the association between earnings and gender, but now using regression with multiple predictors. 

\begin{enumerate}[label=(\alph*)]
\item Perform a linear regression using \verb!statsmodels! with gender and height as predictors. 
\item Provide interpretations for each regression coefficient (like we did in class for the test score example). 
\item Which factor, height or gender is more important based on your analysis? 
\item Based one the fitted model, predict the chance that someone who is not male and is 5.8ft earns more than a male who is the same height? To get a sense for the importance (or lack-thereof) of the height predictor, compare this to the chance that a male earns more than a non-male (regardless of height).
\end{enumerate}
\end{exercise}

\begin{exercise}[Sample distribution  \ding{111}]\label{ex:sampledist}
In the \href{https://colab.research.google.com/drive/1oIRgP_7-c5DGV1D2iz5nj406mZfJxUIG?usp=sharing}{notebook from class}, we wrote code to generate samples from the sample distribution of $(\hat{\beta}_1,\hat{\beta}_2)$ in the model 
\begin{align*}
X_1 &\sim {\rm Normal}(0,1). \\
X_2|X_1 &\sim {\rm Normal}(bX_1,1-b^2)\\
Y|(X_1,X_2) &\sim {\rm Normal}( \beta_1X_1 + \beta_2X_2,\sigma^2) 
\end{align*} 
Specifically, we had a function which takes $\beta_1$, $\beta_2$ and $\beta_0$ as inputs and returns a dataframe where the columns are the samples of $\hat{\beta}_1$ and $\hat{\beta}_2$ respectively. When we plotted the correlation coefficient as a function of $b$ values and estimates the correlation coefficient between $\hat{\beta}_1$ and $\hat{\beta}_2$, it was a decreasing line. 
%Consider the model from Exercise \ref{ex:binnorm}. We will study the sample distribution of the fitted regression coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$. 
%\begin{enumerate}[label=(\alph*)]
%\item Let $q=0.3,b=2,\sigma_{2|1} = 0.3,\beta_1 = 10,\beta_2 = 2,\sigma=0.2$ and $n = 40$ (there are 40 data points). Generate $1000$ simulated data sets with these parameters values, and for each one save the fitted regression coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$. Then make a scatter plot of $\hat{\beta}_2$ vs. $\hat{\beta}_1$. Before doing so, I recommend thinking for a moment about what you expect this to look like. What will the association between these two estimates be? Will it be negative or positive? 
\begin{enumerate}[label=(\alph*)]
\item What would happen if instead of plotting the correlation coefficient, we plotted ${\rm se}(\hat{\beta}_1)$ as a function of $b$? Would it increase? decrease? neither? Note that both $X_1$ and $X_2$ are standardized, so the distribution of $X_1$ values is not changed when we adjust $b$. In answering this question, you can either give a geometric intuition, or do a calculation. You should check your answer with simulations, but you still need to provide a detailed explanation. 
\item Is it possible to have large standard errors on all the $\hat{\beta}_i$ values (measured relative to the true values of course), but still have a large (meaning close to one) value of $R^2$? If so, for what parameter values does this happen? Run simulation(s) to support your answer. 
\end{enumerate}

\end{exercise}


\begin{exercise}[\ding{111}]
Suppose we have a large amount of data from the model 
\begin{align}
X_1 &\sim  {\rm Normal}(0,1)\\
X_2|X_1 &\sim {\rm Normal}(3X_1,1)\\
Y|(X_1,X_2) &\sim {\rm Normal}(X_1 -2 X_2,1)
\end{align}
if a single-predictor linear regression is performed with $Y$ as the response variable and ONLY $X_2$ as the predictor, what is the regression coefficient $\beta_2'$? 

\end{exercise}


\begin{exercise}[\ding{111}]
Consider two binary predictors \(X_1, X_2 \in \{0,1\}\) with joint distribution given by the table of probabilities:

\[
\begin{array}{c|cc}
P(X_1, X_2) & X_2=0 & X_2=1 \\ \hline
X_1=0 & 0.4 & 0.1 \\
X_1=1 & 0.1 & 0.4
\end{array}
\]

Suppose the outcome \(Y\) satisfies the linear model

\[
Y \mid (X_1, X_2) = X_1 + c X_2 + \epsilon 
\]


\begin{itemize}
\item[(a)] Compute the single-predictor regression coefficient $\beta_1'$ (in terms of $c$)
\item[(b)] Compute the single-predictor regression coefficient $\beta_2'$ (in terms of $c$)
\item[(c)] For what values of $c$ does the model exhibit Simpson's paradox? 
\end{itemize}
\end{exercise}


\begin{exercise}[\ding{111}]
Let \(X = (X_1, X_2)^T\) be a bivariate normal vector with mean zero and covariance matrix
\[
\Sigma = \begin{bmatrix} 1 & 2 \\ 2 & 10 \end{bmatrix}.
\]

Suppose the response satisfies
\[
Y = 0.5 X_1 - 1.5 X_2 + \epsilon, \quad \epsilon \sim \mathcal{N}(0,1).
\]

\begin{itemize}
\item[(a)] Compute the single-predictor regression coefficient \(\beta_1'\) for \(Y\) on \(X_1\).
\item[(c)] Comment on whether Simpson’s paradox occurs in this setup.
\end{itemize}
\end{exercise}


\begin{exercise}[\ding{111}]

A sports scientist is studying the relationship between athletes' endurance performance and two predictors: \textbf{weekly training hours} (\(X_1\)) and \textbf{VO2 max} (\(X_2\), in ml/kg/min), measuring their effect on a 10-km run time \(Y\) (in minutes). Data from 100 athletes are collected.

A single predictor linear regression model of 10-km time \(Y\) on weekly training hours \(X_1\) alone yields:

\begin{verbatim}
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.8884      0.285      3.114      0.002       0.322       1.454
x1             1.8436      0.284      6.487      0.000       1.280       2.408
==============================================================================
\end{verbatim}

A second predictor, VO2 max (\(X_2\)), is included in the model, giving:

\begin{verbatim}
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.0170      0.011     94.148      0.000       0.996       1.038
x1             1.0027      0.011     89.354      0.000       0.980       1.025
x2             3.0020      0.011    261.492      0.000       2.979       3.025
==============================================================================
\end{verbatim}
What is the regression coefficient $\beta_{1,2}$ of $X_1$ with $X_2$ as the predictor? 
\end{exercise}


 \bibliographystyle{unsrt}
\bibliography{./../refs.bib}




\end{document}